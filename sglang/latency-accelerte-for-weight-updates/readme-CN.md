# Latency Accelerate for Weight Updates

## å‰è¨€

æœ¬æ–‡æ˜¯ä¸€ç¯‡ debug ç¬”è®°ï¼Œå› æ­¤æ¯”è¾ƒè¯¦ç»†åœ°æè¿°äº†æˆ‘çš„ debug è¿‡ç¨‹ï¼Œå®é™…ä¸Šç»“è®ºéå¸¸ç®€å•ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€æ®µè¯æ€»ç»“å®Œï¼š

1. ä¸ºäº†å‡†ç¡®æµ‹é‡ GPU çš„ latencyï¼Œæˆ‘ä»¬ä»¬è¦åœ¨æµ‹é€Ÿçš„è¯­å¥å‰ååŠ ä¸Šæ— æ•°çš„ `torch.cuda.synchronize()`ï¼Œå¦åˆ™ç»å¸¸ä¼šå‡ºç° CPU è·‘çš„é£èµ·ï¼Œæ—©æ—© print äº†ï¼Œè€Œ GPU è¿˜å¡åœ¨ä¹‹å‰ã€‚å…·ä½“è€Œè¨€ï¼š

```python
torch.cuda.synchronize()
time_begin = time.time()
# ...
torch.cuda.synchronize()
time_end = time.time()
print(f"latency: {time_end - time_begin:.3f}s")
```

2. ä¸ºäº†å‡†ç¡®ä½¿ç”¨ `dist.barrier()`ï¼Œæœ€å¥½æŒ‡å®š `device_ids`ï¼Œå¦åˆ™åœ¨ CI å¯èƒ½è«åå…¶å¦™ä¼šå› ä¸º device error å¡æ­»ã€‚


## èƒŒæ™¯

è´¹åŠ²åƒç•ªåŠ›æ°”ï¼Œæˆ‘ç»ˆäºæˆåŠŸå®ç°äº† `update_parameter_from_distributed` è¿™ä¸ªæ¥å£ã€‚æŒ‰ç…§ advisor çš„æ„æ€ï¼Œè¿™ä¸ªå‡½æ•° OpenRLHF åŸºäº vllm çš„å®ç°ä¸è¶…è¿‡ 50 è¡Œã€‚æŸç§æ„ä¹‰ä¸Šï¼Œæˆ‘çš„å®ç°å¹¶ä¸ç¹çï¼Œåªæ˜¯ç”±äºç¼ºä¹ç»éªŒï¼Œåå¤æŠ˜è…¾äº†ä¸¤å‘¨ã€‚ç»ˆäºï¼Œåˆ° 2024 å¹´æ„Ÿæ©èŠ‚çš„å‰ä¸€å¤©ï¼Œæˆ‘æˆåŠŸè‡ªé¡¶å‘ä¸‹å®ç°äº†å¦‚ä¸‹çš„ä¸‰ä¸ªæ¥å£ï¼š

1. `init_parameter_update_group`
2. `update_parameter_from_distributed`
3. `get_weights_by_parameter_name`

ä¸‰ä¸ªå‡½æ•°ä¸ºäº†è¾¾æˆä¸€ä¸ªåŠŸèƒ½ï¼Œå‰ä¸€ä¸ªç”¨äºå»ºç«‹è¿›ç¨‹ç»„ã€‚æˆ‘ä»¬å‡å®šå°† Training Engine ä¼ é€’çš„ weights æ”¾åœ¨ rank 0 ä¸Šï¼ˆå°½ç®¡ rank 0 ä¸Šå¯èƒ½å¹¶ä¸è¶³ä»¥å­˜ä¸‹æ•´ä¸ªæ¨¡å‹ï¼Œä½†æ˜¯ training engine æ€»å¯ä»¥ä» rank 0 ä¸Šå°† weights ä¼ å‡ºå»ï¼‰ã€‚æ¥ç€ï¼Œæˆ‘ä»¬çš„ sglang server å°†ä¼šå’Œ rank 0 å»ºç«‹è¿›ç¨‹ç»„ï¼Œå¹¶ä» rank 0 ä¸Šå¹¿æ’­å¾—åˆ° weightsï¼Œå¹¶ä¸” load åˆ°æ‰€æœ‰çš„ tensor parall device ä¸Šã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡ `get_weights_by_parameter_name` è¿™ä¸ªå‡½æ•°æ£€æŸ¥ sglang inference engine çš„æ›´æ–°æ˜¯å¦å®Œå–„ã€‚æ³¨æ„åˆ°ï¼Œtraining engine ä¸å¿…ç„¶éœ€è¦å°† model å­˜å‚¨ä¸º huggingface æ ¼å¼ï¼Œäº‹å®ä¸Šå·¥ä¸šç•Œå¤§è§„æ¨¡ä½¿ç”¨çš„å¼•æ“è‚¯å®šæ˜¯æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹åˆ©ç”¨è‡ªèº«çš„æ¨¡å‹æ ¼å¼ï¼Œç„¶åè®­ç»ƒå®Œæˆäº†æ‰å°† checkpoint è½¬ä¸º huggingface æ ¼å¼ç”¨äºå‘å¸ƒã€‚ç„¶è€Œï¼ŒOpenRLHF ä½œä¸ºåå­¦æœ¯ç•Œçš„å¼€æºäº§å“ï¼Œä¼šä½¿ç”¨ huggingface model ä½œä¸ºé€šç”¨çš„ protocolã€‚


<details>
<summary>ä¸ºä»€ä¹ˆä¼šæœ‰ä¸¤ä¸ª engineï¼Ÿ</summary>

è¿™é‡Œéœ€è¦æå‡ºä¸€ä¸ªçœ‹ä¸Šå»å¾ˆæ˜¾ç„¶çš„é—®é¢˜ï¼Œä¸ºä»€ä¹ˆ RLHF æµç¨‹éœ€è¦ training å’Œ inference ä¸¤ä¸ª engineï¼Ÿå¯¹äºå‰è€…ï¼Œä¸»æµç³»ç»Ÿæœ‰éå¸¸å¤šé€‰æ‹©ï¼Œè­¬å¦‚ DeepSpeedï¼Œè€Œåè€…ï¼Œæˆ‘ä»¬å¸Œæœ›æ”¯æŒ SGLangã€‚æ¢å¥è¯è¯´ï¼Œä¸ºä»€ä¹ˆä¸èƒ½ç”¨ training engine åš inferenceï¼Œç”¨ inference engine åš trainingï¼Ÿ

1. training engine åªæœ‰ forwardï¼Œä½†æ˜¯å¾—åˆ° logits ä¹‹åï¼Œæ— è®ºæ˜¯ä¸ºäº† evaluate è¿˜æ˜¯ roll outï¼Œéƒ½éœ€è¦å®é™…è®©æ¨¡å‹åš decodingã€‚decoding å°±å¤§æœ‰æ–‡ç« äº†ï¼ŒSGLang çš„ä¸»è¦è´¡çŒ®æ˜¯ continuous batching and KV cache managementï¼Œå› æ­¤å¤©ç„¶é€‚åˆä¸ºäº†æ•´ä¸ªè®­ç»ƒæµç¨‹åš evaluation æˆ–è€… roll outã€‚
2. åè¿‡æ¥ï¼Œinference engine æ²¡æœ‰ back propagationï¼Œå½“ç„¶ä¸å¯èƒ½åš trainingã€‚ä¸è¿‡ï¼Œinference engine å¯ä»¥ç”¨äºè®¡ç®— KL divergence ä¹ˆï¼Ÿç­”æ›°ï¼Œä¸å¯ï¼Œå› ä¸º KL divergence éœ€è¦ logits çš„ç²¾åº¦è¾ƒé«˜ï¼Œè€Œ inference engine çš„ logits ç²¾åº¦ç›®å‰å¹¶ä¸æ»¡è¶³ï¼ˆä¸æ»¡è¶³çš„åŸå› æˆ‘ä¹Ÿè¿˜åœ¨ç†è§£ï¼‰ã€‚

</details>

æ€»ä¹‹ï¼Œå®ç°äº†è¿™ä¸‰ä¸ªæ¥å£ä¹‹åï¼Œæˆ‘ç»ˆäºæ‰‹å†™äº†å•æµ‹ï¼Œç„¶åæˆåŠŸé€šè¿‡äº†æµ‹è¯•ï¼Œæ•ˆç‡å´ä¸å°½å¦‚äººæ„ã€‚

## æµ‹è¯•æ•ˆæœ

<details>
<summary> å…·ä½“çš„å•æµ‹ </summary>

```python

import gc
import os
import time
import unittest

import numpy as np
import torch
import torch.multiprocessing as mp
from transformers import AutoModelForCausalLM

import sglang as sgl
from sglang.srt.utils import init_custom_process_group
from sglang.test.test_utils import (
    DEFAULT_MODEL_NAME_FOR_TEST,
    DEFAULT_SMALL_MODEL_NAME_FOR_TEST,
)

mp.set_start_method("spawn", force=True)


class TestParameterUpdateGroup(unittest.TestCase):

    @classmethod
    def init_process(
        cls,
        rank,
        world_size,
        param_queue,
        truncate_size,
        state_dict_key_to_shape,
        tp_size,
        model_name,
    ):
        torch.cuda.set_device(rank)
        parameters = [
            "model.embed_tokens.weight",
            "model.layers.0.input_layernorm.weight",
            "model.layers.1.self_attn.q_proj.weight",
            "model.layers.2.self_attn.k_proj.weight",
            "model.layers.3.self_attn.v_proj.weight",
            "model.layers.4.self_attn.o_proj.weight",
            "model.layers.5.mlp.gate_proj.weight",
            "model.layers.6.mlp.up_proj.weight",
            "model.layers.7.mlp.down_proj.weight",
            "model.layers.8.post_attention_layernorm.weight",
            "model.norm.weight",
            "lm_head.weight",
        ]
        print(f"testing model: {model_name}")
        print(f"testing tp size: {tp_size}")
        if rank == 0:
            os.environ["NCCL_CUMEM_ENABLE"] = "0"
            os.environ["NCCL_NVLS_ENABLE"] = "0"

            # åŠ è½½ instruct æ¨¡å‹
            torch.cuda.synchronize()
            time_begin = time.time()
            cls.hf_instruct_model = AutoModelForCausalLM.from_pretrained(
                model_name, torch_dtype="bfloat16"
            ).to("cuda:0")
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"rank {rank} load instruct model time: {time_end - time_begin:.3f}s")

            # åŠ è½½ base æ¨¡å‹
            torch.cuda.synchronize()
            time_begin = time.time()
            base_model_name = model_name.replace("-Instruct", "")
            cls.hf_base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name, torch_dtype="bfloat16"
            ).to("cuda:0")
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"rank {rank} load base model time: {time_end - time_begin:.3f}s")

            cls.hf_instruct_params = []
            cls.hf_base_params = []

            # è·å–å‚æ•°
            torch.cuda.synchronize()
            time_begin = time.time()
            print(f"get parameter in hf instruct model and base model")
            for parameter_name in parameters:
                cls.hf_instruct_params.append(
                    cls.hf_instruct_model.get_parameter(parameter_name)[:truncate_size]
                    .cpu()
                    .detach()
                    .float()
                    .numpy()
                    .tolist()
                )
                cls.hf_base_params.append(
                    cls.hf_base_model.get_parameter(parameter_name)[:truncate_size]
                    .cpu()
                    .detach()
                    .float()
                    .numpy()
                    .tolist()
                )
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"rank {rank} get parameters time: {time_end - time_begin:.3f}s")

            param_queue.put(("hf_instruct_params", cls.hf_instruct_params))
            param_queue.put(("hf_base_params", cls.hf_base_params))

            # åˆå§‹åŒ–è¿›ç¨‹ç»„
            torch.cuda.synchronize()
            time_begin = time.time()
            print(f"rank {rank} init custom process group")
            cls.group = init_custom_process_group(
                backend="nccl",
                init_method="tcp://localhost:65500",
                world_size=world_size,
                rank=rank,
                group_name="test_parameter_update_group",
            )
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"rank {rank} init process group time: {time_end - time_begin:.3f}s")

            # å¹¿æ’­å‚æ•°
            torch.cuda.synchronize()

            print(f"rank {rank} broadcast parameter")

            for parameter_name in state_dict_key_to_shape.keys():
                torch.cuda.synchronize()
                time_begin = time.time()
                torch.distributed.broadcast(
                    cls.hf_base_model.get_parameter(parameter_name),
                    src=0,
                    group=cls.group,
                )
                torch.cuda.synchronize()
                time_end = time.time()
                print(
                    f"rank {rank} broadcast {parameter_name} time: {time_end - time_begin:.3f}s"
                )

            torch.cuda.synchronize()

            del cls.hf_instruct_model
            del cls.hf_base_model
            gc.collect()
            torch.cuda.empty_cache()

        elif rank == 1:
            # åˆå§‹åŒ–å¼•æ“
            torch.cuda.synchronize()
            time_begin = time.time()
            cls.engine = sgl.Engine(
                model_path=model_name,
                random_seed=42,
                base_gpu_id=rank,
                tp_size=tp_size,
            )
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"rank {rank} init engine time: {time_end - time_begin:.3f}s")

            # è·å– instruct å‚æ•°
            torch.cuda.synchronize()
            time_begin = time.time()
            cls.engine_instruct_params = []
            print(f"rank {rank} get parameter in engine instruct model")
            for parameter_name in parameters:
                cls.engine_instruct_params.append(
                    cls.engine.get_weights_by_parameter_name(
                        parameter_name, truncate_size
                    )
                )
            torch.cuda.synchronize()
            time_end = time.time()
            print(
                f"rank {rank} get instruct parameters time: {time_end - time_begin:.3f}s"
            )

            param_queue.put(("engine_instruct_params", cls.engine_instruct_params))

            # åˆå§‹åŒ–å‚æ•°æ›´æ–°ç»„
            torch.cuda.synchronize()
            time_begin = time.time()
            print(f"rank {rank} init parameter update group")
            cls.engine.init_parameter_update_group(
                master_address="localhost",
                master_port="65500",
                rank_offset=1,
                world_size=world_size,
                group_name="test_parameter_update_group",
                backend="nccl",
            )
            torch.cuda.synchronize()
            time_end = time.time()
            print(
                f"rank {rank} init parameter update group time: {time_end - time_begin:.3f}s"
            )

            # æ›´æ–°åˆ†å¸ƒå¼å‚æ•°
            torch.cuda.synchronize()
            time_begin = time.time()
            print(f"rank {rank} update parameter from distributed")
            for parameter_name in state_dict_key_to_shape.keys():
                torch.cuda.synchronize()
                time_begin = time.time()
                cls.engine.update_parameter_from_distributed(
                    parameter_name,
                    dtype=torch.bfloat16,
                    shape=state_dict_key_to_shape[parameter_name],
                    empty_cache=True,
                )
                torch.cuda.synchronize()
                time_end = time.time()
                print(
                    f"rank {rank} update {parameter_name} from distributed time: {time_end - time_begin:.3f}s"
                )

            torch.cuda.synchronize()
            # è·å– base å‚æ•°
            time_begin = time.time()
            cls.engine_base_params = []
            print(f"rank {rank} get parameter in engine base model")
            for parameter_name in parameters:
                cls.engine_base_params.append(
                    cls.engine.get_weights_by_parameter_name(
                        parameter_name, truncate_size
                    )
                )
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"rank {rank} get base parameters time: {time_end - time_begin:.3f}s")

            param_queue.put(("engine_base_params", cls.engine_base_params))
            print(f"rank {rank} shutdown engine")
            cls.engine.shutdown()

    @classmethod
    def setUpClass(cls):
        assert torch.cuda.device_count() >= 2, "At least 2 GPUs are required"
        cls.test_suits = [1]
        cls.model_names = [
            DEFAULT_SMALL_MODEL_NAME_FOR_TEST,
            DEFAULT_MODEL_NAME_FOR_TEST,
        ]

        if torch.cuda.device_count() >= 4:
            cls.test_suits.append(2)

        # åˆå§‹åŒ–æ¯ä¸ªæ¨¡å‹çš„ state_dict_key_to_shape
        cls.model_state_dict_shapes = {}
        for model_name in cls.model_names:
            torch.cuda.synchronize()
            time_begin = time.time()
            model = AutoModelForCausalLM.from_pretrained(
                model_name, torch_dtype="bfloat16"
            ).to("cuda:0")
            state_dict = model.state_dict()
            state_dict_keys = list(state_dict.keys())
            cls.model_state_dict_shapes[model_name] = {
                key: state_dict[key].shape for key in state_dict_keys
            }
            del model
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            time_end = time.time()
            print(
                f"Initialize state dict shapes for {model_name} time: {time_end - time_begin:.3f}s"
            )
            time.sleep(2)

    @classmethod
    def test_init_parameter_update_group(cls):
        truncate_size = 10

        for model_name in cls.model_names:
            print(f"Testing model: {model_name}")
            state_dict_key_to_shape = cls.model_state_dict_shapes[model_name]

            for tp_size in cls.test_suits:
                print(f"test tp_size: {tp_size}")
                param_queue = mp.Queue()
                results = {}

                torch.cuda.synchronize()
                time_begin = time.time()
                context = mp.spawn(
                    cls.init_process,
                    args=(
                        1 + tp_size,
                        param_queue,
                        truncate_size,
                        state_dict_key_to_shape,
                        tp_size,
                        model_name,
                    ),
                    nprocs=2,
                    join=False,
                )

                while len(results) < 4:
                    try:
                        key, value = param_queue.get(timeout=5)
                        results[key] = value
                    except Exception as e:
                        if all(not p.is_alive() for p in context.processes):
                            break

                context.join()
                torch.cuda.synchronize()
                time_end = time.time()
                print(f"Total spawn and join time: {time_end - time_begin:.3f}s")

                if len(results) != 4:
                    raise RuntimeError(f"Expected 4 parameters but got {len(results)}")

                hf_instruct_params = results["hf_instruct_params"]
                hf_base_params = results["hf_base_params"]
                engine_instruct_params = results["engine_instruct_params"]
                engine_base_params = results["engine_base_params"]

                for i in range(len(hf_instruct_params)):
                    assert np.allclose(
                        np.array(hf_instruct_params[i]),
                        np.array(engine_instruct_params[i]),
                    )
                    assert np.allclose(
                        np.array(hf_base_params[i]), np.array(engine_base_params[i])
                    )
                    assert not np.allclose(
                        np.array(hf_instruct_params[i]), np.array(hf_base_params[i])
                    )

                del context
                param_queue.close()
                param_queue.join_thread()
                gc.collect()
                torch.cuda.empty_cache()
                time.sleep(2)


if __name__ == "__main__":
    unittest.main()
```

</details>


ç®€å•æ¥è¯´ï¼Œè¿™ä¸ªæµ‹è¯•çš„é€»è¾‘å¦‚ä¸‹ï¼Œå¯¹äº model ä¸º 8B llama 3.1 å’Œ 1B llama 3.2 åˆ†åˆ«æµ‹è¯•åœ¨ sglang engine çš„ tp ä¸º 1 å’Œ 2 æ—¶çš„æ­£ç¡®æ€§å’Œæ•ˆç‡ï¼š

1. rank 0 (æ¨¡æ‹Ÿ training engine)
- åˆ©ç”¨ huggingface åŠ è½½ instruct model å’Œ base model
- è¯»å–ä»£è¡¨æ€§å‚æ•°ä½œä¸ºéªŒè¯æ ·æœ¬ï¼ˆæ¯ä¸€ç±»å‚æ•°éƒ½åšäº†æŠ½æŸ¥ï¼‰
- åˆå§‹åŒ–è¿›ç¨‹ç»„
- å¹¿æ’­ base model çš„å…¨éƒ¨å‚æ•°

2. rank 1 (SGLang inference engine)
- åˆå§‹åŒ– engineï¼ŒåŠ è½½ instruct model
- è¯»å– instruct model çš„ä»£è¡¨æ€§å‚æ•°
- åˆå§‹åŒ–å‚æ•°æ›´æ–°ç»„
- æ¥æ”¶å¹¶æ›´æ–°å…¨éƒ¨å‚æ•°
- è·å–æ›´æ–°åçš„ base model å‚æ•°è¿›è¡ŒéªŒè¯

æ•´ä½“ä¸Šï¼Œåœ¨æ»¡è¡€ 8 å¡ H100 ä¸Šï¼Œå±…ç„¶æ•´ä½“æµ‹è¯•ç»“æŸéœ€è¦ 431.264sï¼Œæˆ‘æ„Ÿåˆ°éå¸¸è´¹è§£ã€‚æ³¨æ„åˆ°ï¼Œå®é™…ä¸Šçš„æ›´æ–°å‡½æ•°å¦‚ä¸‹ï¼š

<details>
<summary>å®é™…çš„æ›´æ–°å‡½æ•°</summary>

```python

    def update_parameter_from_distributed(self, name, dtype, shape, empty_cache=False):
        """
        Update specific parameter in the model weights online through the process group.

        Args:
            name: the name of the parameter to be updated.
            dtype: the data type of the parameter to be updated.
            shape: the shape of the parameter to be updated.
            empty_cache: whether to empty the cache after updating the parameter.
        """
        target_dtype = (
            dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)
        )
        current_dtype = self.dtype if isinstance(self.dtype, str) else self.dtype
        assert str(target_dtype) == str(
            current_dtype
        ), f"dtype mismatch: target={dtype} vs current model runner={self.dtype}"
        assert (
            self._model_update_group is not None
        ), "model update group must be initialized"

        try:
            weights = torch.empty(shape, dtype=target_dtype, device=self.device)
            torch.distributed.broadcast(weights, src=0, group=self._model_update_group)
            self.model.load_weights([(name, weights)])
            if empty_cache:
                torch.cuda.empty_cache()

            return True, f"Succeeded to update parameter {name} online."

        except Exception as e:
            error_msg = (
                f"Failed to update parameter online: {e}. "
                f"The full weights of the ModelRunner are partially updated. "
                f"Please discard the whole weights."
            )
            logger.error(error_msg)
            return False, error_msg

```

</details>

çœ‹ä¸Šå»æ¯ä¸€æ­¥éƒ½ä¸è¯¥å¾ˆæ…¢ï¼Œä½†æ˜¯å‡ºç°äº†éå¸¸ç¥å¥‡çš„äº‹æƒ…ã€‚åœ¨æˆ‘çš„ uint test ä¸Šï¼Œæˆ‘æŠŠæ¯ä¸ªå‚æ•°çš„æ›´æ–°ç”¨æ—¶éƒ½åœ¨è¿™å‡ è¡Œæ‰“å°å‡ºæ¥äº†ã€‚

```python

            for parameter_name in state_dict_key_to_shape.keys():
                torch.cuda.synchronize()
                time_begin = time.time()
                cls.engine.update_parameter_from_distributed(
                    parameter_name,
                    dtype=torch.bfloat16,
                    shape=state_dict_key_to_shape[parameter_name],
                    empty_cache=True,
                )
                torch.cuda.synchronize()
                time_end = time.time()
                print(
                    f"rank {rank} update {parameter_name} from distributed time: {time_end - time_begin:.3f}s"
                )

```

åŒæ—¶ï¼Œæˆ‘åœ¨æœ€åº•å±‚å®é™…è°ƒç”¨çš„ `update_parameter_from_distributed` å‡½æ•°ä¸­è¯•å›¾æ‰“å°æ¯ä¸€æ­¥çš„ç”¨æ—¶ï¼š

<details>
<summary>æµ‹è¯•æ›´æ–°ä»£ç æ¯ä¸€æ­¥çš„è€—æ—¶</summary>

```python

    def update_parameter_from_distributed(self, name, dtype, shape, empty_cache=False):
        """
        Update specific parameter in the model weights online through the process group.

        Args:
            name: the name of the parameter to be updated.
            dtype: the data type of the parameter to be updated.
            shape: the shape of the parameter to be updated.
            empty_cache: whether to empty the cache after updating the parameter.
        """
        target_dtype = (
            dtype if isinstance(dtype, torch.dtype) else getattr(torch, dtype)
        )
        current_dtype = self.dtype if isinstance(self.dtype, str) else self.dtype
        assert str(target_dtype) == str(
            current_dtype
        ), f"dtype mismatch: target={dtype} vs current model runner={self.dtype}"
        assert (
            self._model_update_group is not None
        ), "model update group must be initialized"

        try:
            torch.cuda.synchronize()
            time_begin = time.time()
            weights = torch.empty(shape, dtype=target_dtype, device=self.device)
            torch.cuda.synchronize()
            time_end = time.time()
            print(
                f"rank {self.tp_rank} {name} create weights time: {time_end - time_begin:.3f}s"
            )
            torch.cuda.synchronize()
            time_begin = time.time()
            torch.distributed.broadcast(weights, src=0, group=self._model_update_group)
            torch.cuda.synchronize()
            time_end = time.time()
            print(
                f"rank {self.tp_rank} {name} broadcast weights time: {time_end - time_begin:.3f}s"
            )
            torch.cuda.synchronize()
            time_begin = time.time()
            self.model.load_weights([(name, weights)])
            torch.cuda.synchronize()
            time_end = time.time()
            print(
                f"rank {self.tp_rank} {name} load weights time: {time_end - time_begin:.3f}s"
            )
            if empty_cache:
                torch.cuda.empty_cache()

            return True, f"Succeeded to update parameter {name} online."

        except Exception as e:
            error_msg = (
                f"Failed to update parameter online: {e}. "
                f"The full weights of the ModelRunner are partially updated. "
                f"Please discard the whole weights."
            )
            logger.error(error_msg)
            return False, error_msg

```

</details>

å¯¹äºæ•´ä¸ªæ›´æ–°å‡½æ•°ï¼Œæˆ‘å‡ ä¹æ€€ç–‘åˆ°äº†æ¯ä¸€æ­¥å¤´ä¸Šã€‚é¦–å…ˆæ˜¯å¼€å¤´çš„å‡ ä¸ªæ£€æŸ¥ç”¨çš„ assertï¼Œæ¥ç€æ˜¯åˆ›å»º weights çš„ empty tensorï¼Œç„¶åæ˜¯å¹¿æ’­ï¼Œæœ€åæ˜¯ load weightsã€‚

æƒŠäººçš„æ˜¯ï¼Œå•ç‹¬æ¯ä¸€æ­¥çš„è€—æ—¶éƒ½æ˜¯ 0.000sï¼Œç„¶è€Œå•æµ‹ä¸­çš„è¿”å›æ—¶é—´å±…ç„¶æ˜¯ 0.032sã€‚æ­¤å¤–ï¼Œ8B model å’Œ 1B model çš„å•æ­¥æ›´æ–°æ—¶é—´å®Œå…¨ä¸€è‡´ã€‚å¤ªæœ‰æ„æ€äº†ï¼Œè¿™æ ·ä»¥æ¥æˆ‘æ›´æ–°æ•´ä¸ª 1B æ¨¡å‹çš„ç”¨æ—¶è¾¾åˆ°äº† 7.047sã€‚è€ƒè™‘åˆ° H100 çš„æ»¡è¡€ NV Link å¸¦å®½å•ä½æ˜¯ TB / sï¼Œè€Œ 1B æ¨¡å‹çš„ weights åœ¨ bf16 ä¹Ÿå°± 2GB å·¦å³ï¼Œè¿™æ ·çš„æ—¶é—´æ¶ˆè€—æ˜¾ç„¶æ˜¯ä¸åˆç†çš„ã€‚

æ‰€ä»¥ï¼Œæ—¶é—´éƒ½å»å“ªå„¿äº†ï¼Ÿ

## æ—¶é—´éƒ½å»å“ªå„¿äº†ï¼Ÿ

å¥½é—®é¢˜ï¼Œå…«åƒä½™æ—¥å¤œå·²ç»åœ¨æˆ‘çš„ç”Ÿå‘½ä¸­æµé€ï¼Œè€Œæˆ‘çš„äººç”Ÿä¹Ÿä¸è¿‡ä¸‰ä¸‡å¤šå¤©ã€‚åˆä¸­æ—¶ï¼Œæ›¾ç»æ•™è¿‡æˆ‘ä¸€æ®µæ—¶é—´çš„æ•°å­¦ç«èµ›çš„ä¸€ä½è€å¸ˆè€å¸ˆå¸¸è¯´ï¼Œâ€œäººç”Ÿå•Šï¼Œä¸è¿‡ä¸‰ä¸‡å¤šå¤©ï¼Œæˆ‘ä¹Ÿæ›¾å¹´è½»è¿‡ï¼Œå“¦è±ï¼Œå°±è€å’¯...â€åå¹´å‰çš„æˆ‘ç»ä¸æ›¾æ„Ÿå—è¿‡æ—¶é—´çš„æµé€ï¼Œç„¶è€Œåå¹´åï¼Œæˆ‘å·²ç»äºŒåäºŒå²ï¼Œæƒ³åˆ°äººç±»ä¸–ç•Œçš„è’è¯å’Œè™šæ— ï¼ŒåŸæ¥æ—¶é—´æ˜¯å¯¹äººçš„æƒ©ç½šã€‚ä¸€æ–¹é¢ï¼Œæˆ‘æ„Ÿå¿µï¼Œæ¯•ç«Ÿæˆ‘åªæœ‰ä¸€ç”Ÿè¿™ä¹ˆçŸ­ï¼Œè®¨å¥½ä»–äººå¯¹æˆ‘è€Œè¨€æ— ç–‘æ˜¯åœ¨æµªè´¹ç”Ÿå‘½ï¼Œå¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘çš„ç”Ÿå‘½çš„å¼€å§‹å’Œç»“æŸéƒ½æ˜¯è™šæ— çš„ï¼Œé‚£ä¹ˆæˆ‘çš„äººç”Ÿåˆ°åº•æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Ÿ

è‡³å°‘ï¼Œæƒ³åŠæ³•å°†è¿™ 7.047s çš„ä¼ è¾“å¼€é”€é™åˆ° 1s ä»¥å†…ï¼Œæ˜¯æˆ‘ç†è§£çš„äººç”Ÿæ„ä¹‰çš„ä¸€éƒ¨åˆ†ã€‚

æˆ‘åˆç†æ€€ç–‘ï¼Œè¿™äº›å¼€é”€æœ‰å¦‚ä¸‹å¯èƒ½ï¼š

1. `https` è¯·æ±‚å¤ªæ…¢äº†ï¼šåœ¨ sglang çš„è®¾è®¡æ¨¡å¼ä¸­ï¼Œæœ‰ä¸¤å±‚ `https` è¯·æ±‚ï¼Œä¸€å±‚æ˜¯æœ€é¡¶å±‚çš„ `RunTime` é€šè¿‡ä¸€ä¸ª fastapi å‘ä¸‹è°ƒç”¨ tokenizer managerï¼Œå¦ä¸€å±‚æ˜¯ `tokenizer manager` é€šè¿‡å¦ä¸€ä¸ª fast api çš„ `https` è¯·æ±‚å‘ `scheduler -> tp worker -> model runner` ä¼ é€’è¯·æ±‚ã€‚

2. python çš„å‡½æ•°ä¼ é€’å¼€é”€å¤ªå¤§äº†ï¼šModel Runner çš„ `update_parameter_from_distributed` å±…ç„¶æ¯ä¸€æ­¥éƒ½æ˜¯ 0.000sï¼Œé‚£ä¹ˆè‡ªé¡¶å‘ä¸‹ï¼Œä» `RunTime` åˆ° `tokenizer manager` å†åˆ° `scheduler -> tp worker -> model runner` æ˜¯å¦å­˜åœ¨å¾ˆå¤§çš„ä¼ é€’å¼€é”€ã€‚ç©¶ç«Ÿæ˜¯å“ªä¸€å±‚æ˜¾è‘—å¢å¤§äº†å¼€é”€ï¼Ÿ

3. æˆ‘æ²¡æœ‰å¼‚æ­¥æ›´æ–°å‚æ•°ï¼šå®é™…ä¸Šï¼Œ`update_parameter_from_distributed` å¹¶ä¸ä¼šé‡å¤å†™åŒä¸€ç‰‡ weightsï¼Œä¼¼ä¹å¼‚æ­¥æ˜¯ä¸€ä¸ªè§£ã€‚

4. update çš„æ—¶å€™ï¼Œæ˜¯ä¸æ˜¯åœ¨ blocking çŠ¶æ€ä¸‹ï¼Ÿè¯•è¯•åª launch kernelï¼Œè¿™æ ·å¯ä»¥å…¨éƒ¨ overlap èµ·æ¥ï¼ˆfrom advisorï¼‰

5. nccl å¤ªæ…¢äº†ï¼šè¿™æ˜¯æˆ‘è§‰å¾—å¾ˆä¸ç°å®çš„ï¼Œå› ä¸ºæˆ‘çš„æµ‹è¯•æœºå™¨æ˜¯ NVDA æä¾›çš„æ»¡è¡€ H100ã€‚

æ— æ‰€è°“ï¼Œæˆ‘å…ˆè¿›è¡Œè¿™æ ·ä¸€ä¸ªæµ‹è¯•ï¼š

```python

            torch.cuda.synchronize()
            time_begin = time.time()
            print(
                f"start to update model_name {model_name} rank {rank} parameter from distributed"
            )
            for parameter_name in state_dict_key_to_shape.keys():
                torch.cuda.synchronize()
                time_begin = time.time()
                cls.engine.update_parameter_from_distributed(
                    parameter_name,
                    dtype=torch.bfloat16,
                    shape=state_dict_key_to_shape[parameter_name],
                    empty_cache=True,
                )
                torch.cuda.synchronize()
                time_end = time.time()
                print(
                    f"model_name {model_name} rank {rank} update {parameter_name} {state_dict_key_to_shape[parameter_name]} from distributed time: {time_end - time_begin:.3f}s"
                )
            torch.cuda.synchronize()
            time_end = time.time()
            print(
                f"fully update model_name {model_name} rank {rank} parameter from distributed time: {time_end - time_begin:.3f}s"
            )

```

è®©æˆ‘çœ‹çœ‹ï¼Œä¼ è¾“æ•ˆç‡ç©¶ç«Ÿå’Œä»€ä¹ˆæœ‰å…³ç³»ã€‚

```bash
model_name meta-llama/Llama-3.1-8B-Instruct rank 1 update lm_head.weight torch.Size([128256, 4096]) from distributed time: 0.055s
fully update model_name meta-llama/Llama-3.1-8B-Instruct rank 1 parameter from distributed time: 0.055s
```

Wellï¼Œçœ‹ä¸Šå»ç»“æœä¸å¤ªå¦™ï¼Œè¿™ä¼¼ä¹æ˜¯ python ç¼–è¯‘å™¨çš„ä½œç”¨åŸŸé—®é¢˜ï¼ˆæˆ‘æ²¡å­¦å¥½ç¼–åŸï¼Œä¹Ÿå°±åªçŸ¥é“è¿™ä¸ªè¯äº†ï¼‰ã€‚


æˆ‘ä»¬æ¢ç§æ–¹å¼æ¥ print æ—¶é—´ï¼š

```python

            torch.cuda.synchronize()
            time_begin_fully_update = time.time()
            print(
                f"start to update model_name {model_name} rank {rank} parameter from distributed"
            )
            for parameter_name in state_dict_key_to_shape.keys():
                torch.cuda.synchronize()
                time_begin_single_update = time.time()
                cls.engine.update_parameter_from_distributed(
                    parameter_name,
                    dtype=torch.bfloat16,
                    shape=state_dict_key_to_shape[parameter_name],
                    empty_cache=True,
                )
                torch.cuda.synchronize()
                time_end_single_update = time.time()
                print(
                    f"model_name {model_name} rank {rank} update {parameter_name} {state_dict_key_to_shape[parameter_name]} from distributed time: {time_end_single_update - time_begin_single_update:.3f}s"
                )
            torch.cuda.synchronize()
            time_end_fully_update = time.time()
            print(
                f"fully update model_name {model_name} rank {rank} parameter from distributed time: {time_end_fully_update - time_begin_fully_update:.3f}s"
            )

```

è¿™æ ·åº”è¯¥ä¸ä¼šç›¸äº’è¦†ç›–äº†ã€‚æ‹¿åˆ°çš„ç»“æœå¾ˆæœ‰è¶£ï¼š

```bash

model_name meta-llama/Llama-3.2-1B-Instruct rank 1 update model.embed_tokens.weight torch.Size([128256, 2048]) from distributed time: 1.620s

rank 0 broadcast model.embed_tokens.weight time: 1.612s

model_name meta-llama/Llama-3.2-1B-Instruct rank 1 update model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048]) from distributed time: 0.034s

rank 0 broadcast model.layers.0.self_attn.o_proj.weight time: 0.000s

model_name meta-llama/Llama-3.2-1B-Instruct rank 1 update model.layers.0.mlp.gate_proj.weight torch.Size([8192, 2048]) from distributed time: 0.032s

rank 0 broadcast model.layers.0.mlp.gate_proj.weight time: 0.000s

model_name meta-llama/Llama-3.2-1B-Instruct rank 1 update model.layers.1.self_attn.k_proj.weight torch.Size([512, 2048]) from distributed time: 0.031s

rank 0 broadcast model.layers.1.self_attn.k_proj.weight time: 0.000s
```

è¿™äº›å¤ªç„å­¦äº†ï¼Œæˆ‘ä¸€æ—¶æ‹ä¸å‡ºé—®é¢˜ï¼Œåƒæäº†é«˜ä¸­æ—¶ç‰©ç†ç«èµ›çš„åŒå­¦åšçš„ç‰©ç†å®éªŒæŠ¥å‘Š...

ä½†æ˜¯æˆ‘è¿˜æ˜¯è§‚å¯Ÿåˆ°äº†è¿™ä¹ˆä¸€ä»¶äº‹æƒ…ï¼š

```bash
rank 0 init process group time: 44.275s
rank 1 init parameter update group time: 0.005s
```

æœ‰ç‚¹é€†å¤©ï¼Œprocess group çš„åˆ›ç«‹ç»å¯¹æ˜¯åŒæ­¥çš„ï¼Œä½†æ˜¯ä¸¤ä¸ª process group çš„åˆ›å»ºæ—¶é—´å±…ç„¶å·®äº† 44sã€‚æˆ‘æ„Ÿåˆ°éå¸¸è´¹è§£ï¼Œé‚åšå¦‚ä¸‹æµ‹è¯•ï¼š

<details>
<summary>process group åˆ›å»ºæ—¶é—´</summary>

```python
import time
import unittest
import torch
import torch.multiprocessing as mp
from sglang.srt.utils import init_custom_process_group
from sglang.test.test_utils import (
    DEFAULT_MODEL_NAME_FOR_TEST,
    DEFAULT_SMALL_MODEL_NAME_FOR_TEST,
)

mp.set_start_method("spawn", force=True)

class TestProcessGroupInit(unittest.TestCase):
    @classmethod
    def init_process(cls, rank, world_size):
        torch.cuda.set_device(rank)
        
        if rank == 0:
            # åˆå§‹åŒ–è¿›ç¨‹ç»„
            print(f"rank {rank} init custom process group")
            torch.cuda.synchronize()
            time_begin = time.time()
            group = init_custom_process_group(
                backend="nccl",
                init_method="tcp://localhost:65500",
                world_size=world_size,
                rank=rank,
                group_name="test_process_group",
            )
            
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"rank {rank} init process group time: {time_end - time_begin:.3f}s")

        elif rank == 1:
            # åˆå§‹åŒ–å¼•æ“çš„è¿›ç¨‹ç»„
            print(f"rank {rank} init parameter update group")
            torch.cuda.synchronize()
            time_begin = time.time()
            from sglang import Engine
            engine = Engine(
                model_path=DEFAULT_SMALL_MODEL_NAME_FOR_TEST,  # ä½¿ç”¨å°æ¨¡å‹æµ‹è¯•
                random_seed=42,
                base_gpu_id=rank,
                tp_size=1,
            )
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"rank {rank} init engine time: {time_end - time_begin:.3f}s")
            torch.cuda.synchronize()
            time_begin = time.time()
            engine.init_parameter_update_group(
                master_address="localhost",
                master_port="65500",
                rank_offset=1,
                world_size=world_size,
                group_name="test_process_group",
                backend="nccl",
            )
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"rank {rank} init process group time: {time_end - time_begin:.3f}s")
            
            engine.shutdown()

    def test_process_group_init(self):
        assert torch.cuda.device_count() >= 2, "éœ€è¦è‡³å°‘2ä¸ªGPU"
        
        torch.cuda.synchronize()
        time_begin = time.time()
        
        context = mp.spawn(
            self.init_process,
            args=(2,),  # world_size = 2
            nprocs=2,
            join=True
        )
        
        torch.cuda.synchronize()
        time_end = time.time()
        print(f"æ€»è€—æ—¶: {time_end - time_begin:.3f}s")

if __name__ == "__main__":
    unittest.main()
```

</details>

å¾—åˆ°çš„ç»“æœå¦‚ä¸‹ï¼š

```bash
rank 1 init engine time: 20.817s
rank 1 init process group time: 0.014s
rank 0 init process group time: 20.934s
```

okayï¼Œç¡®å®åˆ›å»ºé€šè®¯ç»„éå¸¸å¿«ï¼Œrank 0 å¡ä½çš„åŸå› æ˜¯è¦å’Œ rank 1 çš„ engine åŒæ­¥ï¼Œè€Œ engine å¯åŠ¨è€—æ—¶ 20sï¼Œå®é™…ä¸Š process group çš„åˆ›å»ºæ—¶é—´å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚

æœ‰äº†è¿™ä¸ªæ€è·¯ï¼Œæˆ‘å†³å®šæŠŠæˆ‘å¤æ‚çš„æµ‹ä¾‹ç®€åŒ–ä¸‹ï¼Œä¸è¯»å–å‚æ•°ï¼Œåªæµ‹è¯•æ›´æ–°æ—¶é—´ï¼Œä¸ºäº†é¿å…å¤ªå¤šç¹ççš„åŒæ­¥å½±å“æˆ‘çš„æµ‹é€Ÿï¼š

<details>
<summary>åªæµ‹è¯• broad cast and update çš„æ—¶é—´</summary>

```python
import gc
import os
import time
import unittest
import torch
import torch.multiprocessing as mp
from transformers import AutoModelForCausalLM
import sglang as sgl
from sglang.test.test_utils import (
    DEFAULT_MODEL_NAME_FOR_TEST,
    DEFAULT_SMALL_MODEL_NAME_FOR_TEST,
)
from sglang.srt.utils import init_custom_process_group
mp.set_start_method("spawn", force=True)

class TestParameterUpdateLatency(unittest.TestCase):
    @classmethod
    def init_process(cls, rank, world_size, param_queue, state_dict_key_to_shape, tp_size, model_name):
        torch.cuda.set_device(rank)
        print(f"Testing model: {model_name}")
        
        if rank == 0:
            os.environ["NCCL_CUMEM_ENABLE"] = "0"
            os.environ["NCCL_NVLS_ENABLE"] = "0"
            
            # åˆå§‹åŒ–è¿›ç¨‹ç»„
            torch.cuda.synchronize()
            time_begin = time.time()
            cls.group = init_custom_process_group(
                backend="nccl",
                init_method="tcp://localhost:65500",
                world_size=world_size,
                rank=rank,
                group_name="test_parameter_update_group",
            )
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"Rank {rank} init process group time: {time_end - time_begin:.3f}s")
            
            # å¹¿æ’­å‚æ•°
            torch.cuda.synchronize()
            time_begin_broadcast = time.time()
            for name, shape in state_dict_key_to_shape.items():
                torch.cuda.synchronize()
                time_begin = time.time()
                weights = torch.ones(shape, dtype=torch.bfloat16, device=f"cuda:{rank}")
                torch.distributed.broadcast(weights, src=0, group=cls.group)
                torch.cuda.synchronize()
                time_end = time.time()
                print(f"Rank {rank} broadcast {name} {shape} time: {time_end - time_begin:.3f}s")
            torch.cuda.synchronize() 
            time_end_broadcast = time.time()
            print(f"Rank {rank} broadcast all parameters time: {time_end_broadcast - time_begin_broadcast:.3f}s")
            
            param_queue.put(("rank0_done", True))

        elif rank == 1:
            # åˆå§‹åŒ–å¼•æ“
            torch.cuda.synchronize()
            time_begin = time.time()
            cls.engine = sgl.Engine(
                model_path=model_name,
                random_seed=42,
                base_gpu_id=rank,
                tp_size=tp_size,
            )
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"Rank {rank} init engine time: {time_end - time_begin:.3f}s")
            
            # åˆå§‹åŒ–å‚æ•°æ›´æ–°ç»„
            torch.cuda.synchronize()
            time_begin = time.time()
            cls.engine.init_parameter_update_group(
                master_address="localhost",
                master_port="65500",
                rank_offset=1,
                world_size=world_size,
                group_name="test_parameter_update_group",
                backend="nccl",
            )
            torch.cuda.synchronize()
            time_end = time.time()
            print(f"Rank {rank} init parameter update group time: {time_end - time_begin:.3f}s")
            
            # æ›´æ–°å‚æ•°å¹¶æµ‹é‡æ—¶é—´
            torch.cuda.synchronize()
            time_begin_update = time.time()
            for name, shape in state_dict_key_to_shape.items():
                torch.cuda.synchronize()
                time_begin = time.time()
                cls.engine.update_parameter_from_distributed(
                    name,
                    dtype=torch.bfloat16,
                    shape=shape,
                    empty_cache=True
                )
                torch.cuda.synchronize()
                time_end = time.time()
                print(f"Rank {rank} update {name} {shape} time: {time_end - time_begin:.3f}s")
            torch.cuda.synchronize()
            time_end_update = time.time()
            print(f"Rank {rank} update all parameters time: {time_end_update - time_begin_update:.3f}s")
            
            param_queue.put(("rank1_done", True))
            cls.engine.shutdown()

    @classmethod
    def setUpClass(cls):
        assert torch.cuda.device_count() >= 2, "At least 2 GPUs are required"
        cls.test_suits = [1]
        cls.model_names = [
            DEFAULT_SMALL_MODEL_NAME_FOR_TEST,
            DEFAULT_MODEL_NAME_FOR_TEST,
        ]

        if torch.cuda.device_count() >= 4:
            cls.test_suits.append(2)

        # åˆå§‹åŒ–æ¯ä¸ªæ¨¡å‹çš„ state_dict_key_to_shape
        cls.model_state_dict_shapes = {}
        for model_name in cls.model_names:
            torch.cuda.synchronize()
            time_begin = time.time()
            model = AutoModelForCausalLM.from_pretrained(
                model_name, torch_dtype="bfloat16"
            ).to("cuda:0")
            state_dict = model.state_dict()
            cls.model_state_dict_shapes[model_name] = {
                key: state_dict[key].shape for key in state_dict.keys()
            }
            del model
            gc.collect()
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            time_end = time.time()
            print(
                f"Initialize state dict shapes for {model_name} time: {time_end - time_begin:.3f}s"
            )

    def test_parameter_update_latency(self):
        for model_name in self.model_names:
            print(f"Testing model: {model_name}")
            state_dict_key_to_shape = self.model_state_dict_shapes[model_name]

            for tp_size in self.test_suits:
                print(f"test tp_size: {tp_size}")
                world_size = 1 + tp_size
                param_queue = mp.Queue()
                results = {}
                
                torch.cuda.synchronize()
                time_begin = time.time()
                
                context = mp.spawn(
                    self.init_process,
                    args=(world_size, param_queue, state_dict_key_to_shape, tp_size, model_name),
                    nprocs=2,
                    join=False
                )

                while len(results) < 2:
                    try:
                        key, value = param_queue.get(timeout=5)
                        results[key] = value
                    except Exception as e:
                        if all(not p.is_alive() for p in context.processes):
                            break

                context.join()
                torch.cuda.synchronize()
                time_end = time.time()
                print(f"Total time for {model_name}: {time_end - time_begin:.3f}s")
                
                if len(results) != 2:
                    raise RuntimeError(f"Expected 2 results but got {len(results)}")
                
                del context
                param_queue.close()
                param_queue.join_thread()
                gc.collect()
                torch.cuda.empty_cache()

if __name__ == "__main__":
    unittest.main()
```

</details>

è¿™ä¸€æ¬¡ï¼Œæˆ‘æ‹¿åˆ°å¾ˆå¤šæœ‰è¶£çš„äº‹æƒ…ï¼š

1. update parameter çš„æ—¶é—´å’Œä¸Šæ¬¡å¤æ‚çš„æµ‹ä¾‹å‡ ä¹ä¸€æ ·ï¼›
2. ModelRunner å®é™…ä¸Šçš„æ›´æ–°æ—¶é—´éå¸¸å¿«ï¼Œä½†æ˜¯æ¥å£è¿”å›çš„é€Ÿåº¦å¾ˆæ…¢ï¼›

```bash
ModelRunner update model.layers.0.self_attn.q_proj.weight time: 0.001s
Rank 1 update model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048]) time: 0.033s
Rank 0 broadcast model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048]) time: 0.001s
```

3. `model.embed_tokens.weight torch.Size([128256, 2048])` å‚æ•°å¼‚å¸¸çš„æ…¢ï¼Œè€Œä¸”æ…¢çš„å¾ˆåŒæ­¥ï¼š

```bash
Rank 0 broadcast model.embed_tokens.weight torch.Size([128256, 2048]) time: 1.812s
Rank 1 update model.embed_tokens.weight torch.Size([128256, 2048]) time: 1.819s
ModelRunner update model.embed_tokens.weight time: 1.786s
```

4. `model.layers.12.mlp.up_proj.weight torch.Size([8192, 2048])` åœ¨ Model Runner ä¸Šæ­£å¸¸ï¼Œbroadcast ä¼¼ä¹å¡é¡¿äº†ï¼Œè€Œæœ€åæ•´ä½“çš„ update æ—¶é—´å’Œå…¶ä»– update æ—¶é—´å‡ ä¹ä¸€è‡´ï¼š

```bash
ModelRunner update model.layers.12.mlp.up_proj.weight time: 0.001s
Rank 0 broadcast model.layers.12.mlp.up_proj.weight torch.Size([8192, 2048]) time: 0.162s
Rank 1 update model.layers.12.mlp.up_proj.weight torch.Size([8192, 2048]) time: 0.032s
```

`embed_tokens.weight` å’Œ `up_proj.weight` ä¸å¤ªå¥½è§£å†³ï¼Œä½†æ˜¯æˆ‘æ˜æ˜¾æ„Ÿå—åˆ°äº†å…¶å®åœ¨ `ModelRunner` ä¸Šï¼Œbroad cast å’Œ update çš„æ—¶é—´å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œä½†æ˜¯å®é™…å›ä¼ çš„æ—¶é—´å´å¾ˆé•¿ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬æ¯ä¸€å±‚éƒ½æ‰“å°ä¸€æ¬¡æ—¶é—´ï¼Œçœ‹çœ‹ç©¶ç«Ÿæ˜¯å“ªå„¿æ…¢äº†ä¸‹æ¥ã€‚å…·ä½“æ¥è¯´ï¼Œä» `Engine -> scheduler -> tp worker -> model runner` çš„æ¯ä¸€å±‚éƒ½æ‰“å°ä¸€æ¬¡æ—¶é—´ï¼Œçœ‹çœ‹ç©¶ç«Ÿæ˜¯å“ªå„¿æ…¢äº†ä¸‹æ¥ã€‚

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘çœ‹åˆ°å‡ è¡Œï¼Œç¬é—´å°±æœ‰äº†æ„Ÿè§‰ï¼š

```python
async def update_parameter_from_distributed(
    self,
    obj: UpdateParameterFromDistributedReqInput,
    request: Optional[fastapi.Request] = None,
):
    torch.cuda.synchronize()
    time_begin = time.time()
    if self.to_create_loop:
        self.create_handle_loop()
    if not self.model_update_lock.locked():

        async with self.model_update_lock:
            # wait for the previous update requests to finish
            for i in range(3):
                while len(self.rid_to_state) > 0:
                    await asyncio.sleep(0.001)
                # FIXME: We add some sleep here to avoid some race conditions.
                # We can use a read-write lock as a better fix.
                await asyncio.sleep(0.01)

            self.send_to_scheduler.send_pyobj(obj)
            self.parameter_update_result = asyncio.Future()

            if self.server_args.dp_size == 1:
                result = await self.parameter_update_result
                torch.cuda.synchronize()
                time_end = time.time()
                print(
                    f"In tokenizer manager: update parameter from distributed time: {obj.name} {obj.shape} {time_end - time_begin:.3f}s"
                )
                return result.success, result.message
            else:  # self.server_args.dp_size > 1
                self.parameter_update_tmp = []
                result = await self.parameter_update_result
                all_success = all([r.success for r in result])
                all_message = [r.message for r in result]
                all_message = " | ".join(all_message)
                return all_success, all_message

    else:
        logger.error(
            f"Another parameter update is in progress in tokenizer manager"
        )
        return (
            False,
            "Another parameter update is in progress. Please try again later.",
        )
```

è¿™ä¸‰ä¸ª `await asyncio.sleep(0.01)` ä¸æ˜¯ä¸€çœ¼å¯¼è‡´äº† `0.03` çš„ update latency å—ï¼Ÿæˆ‘è¯•å›¾å»æ‰ï¼Œå¹¶ä¸” print å‡ºæ¥ã€‚æœç„¶ï¼Œè¿™æ¬¡å¾ˆå¿«æ—¶é—´å°±é™ä¸‹æ¥äº†ï¼š

```bash
fully update model_name meta-llama/Llama-3.2-1B-Instruct rank 1 parameter from distributed time: 2.202s
```

è™½ç„¶é€Ÿåº¦å¿«äº†å¾ˆå¤šï¼Œä½†æ˜¯è¿˜æ˜¯å¤§äº 1sï¼Œè€Œä¸”ç»§ç»­è§‚å¯Ÿåˆ°äº†è¿™ä¸ª `model.embed_tokens.weight torch.Size([128256, 2048])` å æ®äº†è¶…è¿‡ 1.6s çš„æ—¶é—´ï¼Œç”šè‡³æ˜¯ä» broadcast é‚£ä¸€æ­¥å°±å¼€å§‹è¶…è¿‡äº† 1.6sã€‚æ˜¯å› ä¸ºç¬¬ä¸€ä¸ªå‚æ•°çš„ broad cast éœ€è¦ init NCCL å¾ˆæ…¢ï¼Œè¿˜æ˜¯å°±è¿™ä¸ªå‚æ•°å¾ˆæ…¢å‘¢ï¼Ÿæˆ‘ä»¬è·³è¿‡è¿™ä¸ªå‚æ•°ï¼Œç›´æ¥ä» `[1:]` å¼€å§‹ï¼Œçœ‹çœ‹ç»“æœå¦‚ä½•ï¼š


```bash
In server: update parameter from distributed time: model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048]) 0.000s
In tokenizer manager: update parameter from distributed time: model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048]) 1.726s
In server time function update parameter from distributed time: model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048]) 1.726s
model_name meta-llama/Llama-3.2-1B-Instruct rank 1 update model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048]) from distributed time: 1.727s
```

å¾ˆæœ‰æ„æ€ï¼Œå°±æ˜¯ç¬¬ä¸€ä¸ªè¢«å¹¿æ’­çš„å‚æ•°å¾ˆæ…¢ï¼Œå…¶ä»–å‚æ•°éƒ½å¾ˆå¿«ã€‚è¿™æ˜¯å› ä¸ºæ²¡æœ‰åŒæ­¥ä¹ˆï¼Ÿæˆ‘å†³å®šåŠ ä¸ª barrier è¯•è¯•åŒæ­¥ä¸€æ¬¡ï¼š

```bash
Rank 1 before barrier
Rank 1 after barrier
In server: update parameter from distributed time: model.embed_tokens.weight torch.Size([128256, 2048]) 0.000s
In tokenizer manager: update parameter from distributed time: model.embed_tokens.weight torch.Size([128256, 2048]) 1.444s
In server time function update parameter from distributed time: model.embed_tokens.weight torch.Size([128256, 2048]) 1.444s
Rank 1 update model.embed_tokens.weight torch.Size([128256, 2048]) time: 1.445s
```

çœ‹ä¸Šå»è¿˜æ˜¯å¾ˆå¯„ï¼Œçš„ç¡®æ˜¯ç¬¬ä¸€æ¬¡é€šè®¯çš„ç”¨æ—¶ç‰¹åˆ«é•¿ã€‚ä½†æ˜¯ä¼¼ä¹ä¹Ÿæ²¡é‚£ä¹ˆå¯„ï¼Œæˆ‘è½¬æ‰‹é—®äº†ä¸‹ gptï¼Œè²Œä¼¼ç¬¬ä¸€æ¬¡é€šè®¯çš„å»ºç«‹ä¸€å®šæ˜¯æ…¢çš„ï¼Œè€Œæˆ‘å¯ä»¥åœ¨ init process group çš„æ—¶å€™å°± barrier ä¸€æ¬¡ï¼ˆä¸€æ¬¡ barrier å®é™…ä¸Šç­‰ä»·äºä¸€æ¬¡å°çš„ all reduceï¼‰ï¼Œçœ‹çœ‹æ­¤åçš„æ•ˆæœå¦‚ä½•ã€‚

...

å¤§åŠŸå‘Šæˆï¼Œåœ¨æˆ‘çš„æœ¬åœ°æœºå™¨ä¸Šï¼Œ1B æ¨¡å‹æ›´æ–°æ—¶é—´é™åˆ°äº† 0.5s å·¦å³ï¼Œ8B æ¨¡å‹é™åˆ°äº† 0.6s å·¦å³ã€‚æƒ³è§å¤§éƒ¨åˆ†çš„å¼€é”€å…¶å®ä¹Ÿä¸æ˜¯é€šè®¯ ğŸ˜‚

PSï¼šåœ¨ process group init ä¹‹åé©¬ä¸Š warm up ä¸€æ¬¡æ˜¯éå¸¸å¸¸è§çš„ï¼Œç„¶åæˆ‘å‘ç°å¾ˆæœ‰è¶£çš„äº‹æƒ…ï¼Œç›´æ¥ç”¨ `dist.barrier()` ä¸æŒ‡å®š devices_id çš„è¯ï¼Œä¼šåœ¨ CI ä¸Šå› ä¸º device error å¡æ­»ï¼Œä½†æ˜¯æœ¬åœ°ä¸ä¼šï¼Œæ‰€ä»¥ä¸€ä¸ªæ›´å¥½çš„å°è¯•æ˜¯ï¼š`dist.barrier(device_ids=[0], group=pg)`
