# å­¦ä¹ è·¯å¾„

ä¸ºäº†ç»™ OpenRLHF å†™ä¸€ä¸ª `weight_update` æ¥å£ï¼Œæ€œæ‚¯ç»™æˆ‘è¯´ï¼Œâ€ä½ åªéœ€è¦å­¦ä¹  `torch.dist`ã€‚â€æˆ‘å½“æ—¶ä¸€å¬ï¼Œâ€œæˆ‘æ€ä¹ˆè®°å¾—æœ‰ä¸ª torch çš„æ¥å£æ˜¯è®¡ç®—è·ç¦»çš„ï¼Œå°±å«åš `torch.dist` å‘¢ï¼Ÿâ€ç„¶åä»–è¯´ï¼Œâ€œå®é™…ä¸Šæ˜¯ `torch.distributed`ã€‚â€å“„å ‚å¤§ç¬‘...

æ— æ‰€è°“ï¼Œæˆ‘ç¡®å®è¦å­¦ä¸‹ `torch.distributed`ï¼š

1. Learn torch.distributed https://pytorch.org/docs/stable/distributed.html
2. How to create a communication group.
3. How to broadcast a tensor.  8 GPUs, 1 process per GPU, 8 processes. Broadcast a torch tensor from 1 GPU to the other 7 GPUs with torch.distributed (nccl backend).

çœ‹å®Œäº†å­¦ä¹ ç›®æ ‡ï¼Œæˆ‘åˆé—®äº†é—®æˆ‘å·¥ä½æ—è¾¹çš„è€å“¥ï¼Œå¦‚ä½•å­¦ä¹  `torch.distributed`ã€‚ä»–è¯´è¯»æºç ï¼Œè€Œæˆ‘æœ¬æ¥æƒ³å¯¹ç€æºç ç¡¬å­¦çš„ï¼Œç»“æœçœ‹äº†å‡ çœ¼ï¼Œç›´æ¥æ”¾å¼ƒ ğŸ˜‚ã€‚ç½‘ä¸ŠæŸ¥äº†æŸ¥ï¼Œä¹Ÿæ²¡æœ‰å¾ˆå¥½çš„æ•™ç¨‹ã€‚æ— æ‰€è°“ï¼Œæˆ‘ä¼šå‡ºæ‰‹ï¼é—®äº†é—® claudeï¼Œå­¦ï¼

## `torch.distributed` Learning Thread

1. åŸºç¡€æ¦‚å¿µ

- è¿›ç¨‹ç»„(Process Group) - åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„åŸºæœ¬é€šä¿¡å•ä½

- åç«¯(Backend) - é‡ç‚¹å…³æ³¨ NCCL åç«¯ï¼Œå› ä¸ºä½ è¦ç”¨ GPU é€šä¿¡

- rank - è¿›ç¨‹ç¼–å·ï¼Œç”¨äºæ ‡è¯†ä¸åŒ GPU ä¸Šçš„è¿›ç¨‹

- world size - æ€»è¿›ç¨‹æ•°ï¼Œåœ¨ä½ çš„åœºæ™¯ä¸­æ˜¯ 8

2. æ ¸å¿ƒ API å­¦ä¹ é¡ºåº

- åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ `init_process_group` 

- åˆ›å»ºè‡ªå®šä¹‰é€šè®¯ç»„ `new_group`

## ä¸»æµé€šè®¯æ¥å£

1. ç‚¹åˆ°ç‚¹é€šè®¯ vs é›†åˆé€šè®¯
2. `send` and `recv`
3. `all_reduce` and `all_gather`
4. `broadcast`
5. `scatter`

# `torch.distributed`

## torch ä¸­çš„åˆ†å¸ƒå¼è®¡ç®—

ä¸ºä»€ä¹ˆéœ€è¦åˆ†å¸ƒå¼è®¡ç®—ï¼Œè‚¯å®šä¸ç”¨æˆ‘è§£é‡Š ğŸ˜‚ã€‚è€Œ `torch.distributed` æ˜¯ PyTorch ä¸­ä¸“é—¨ä¸ºåˆ†å¸ƒå¼è®­ç»ƒè®¾è®¡çš„æ¨¡å—ï¼Œæä¾›äº†åœ¨å¤šä¸ª GPU æˆ–èŠ‚ç‚¹é—´è¿›è¡Œæ•°æ®å’Œæ¨¡å‹å‚æ•°é€šä¿¡çš„å·¥å…·ã€‚ä¸ä¼ ç»Ÿçš„ `torch` å‡½æ•°ä¸åŒï¼Œ`torch.distributed` å…³æ³¨çš„æ˜¯å¦‚ä½•åœ¨å¤šè®¾å¤‡ä¸Šæœ‰æ•ˆåè°ƒå’Œå…±äº«æ•°æ®ï¼Œä»¥ä¾¿å„è®¾å¤‡åœ¨ä¸åŒçš„è®­ç»ƒä»»åŠ¡ä¸­ååŒå·¥ä½œã€‚`torch.distributed` æä¾›äº†é€šä¿¡æ¥å£ï¼Œå…è®¸ç”¨æˆ·åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸­å®ç°å‚æ•°åŒæ­¥ã€æ¢¯åº¦æ±‡æ€»ã€å¹¿æ’­ç­‰æ“ä½œï¼Œä¿è¯æ‰€æœ‰è®¾å¤‡åœ¨æ¯ä¸€è½®è®­ç»ƒä¸­éƒ½ä¿æŒç›¸åŒçš„æ¨¡å‹çŠ¶æ€ã€‚

ä¸æ­¤ç›¸åï¼Œæ™®é€šçš„ `torch` å‡½æ•°é»˜è®¤æ˜¯åŸºäºå•è¿›ç¨‹ã€å•è®¾å¤‡è®¾è®¡çš„ï¼Œå³ä½¿æ˜¯å¤š GPU çš„æƒ…å½¢ï¼Œæ™®é€šçš„ PyTorch ä¹Ÿåªèƒ½æ§åˆ¶ä¸€ä¸ªè¿›ç¨‹åœ¨å¤šä¸ªè®¾å¤‡ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè€Œæ— æ³•æ”¯æŒå¤šä¸ªè¿›ç¨‹åœ¨å¤šä¸ªè®¾å¤‡ä¸Šåä½œã€‚`torch.distributed` æä¾›äº†ä¸€ç§é«˜çº§æŠ½è±¡ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥è½»æ¾ç®¡ç†å¤šä¸ªè®¾å¤‡æˆ–èŠ‚ç‚¹çš„ååŒå·¥ä½œã€‚

åŸºäºåˆ†å¸ƒå¼è®¡ç®—å¯ä»¥æ„é€ åˆ†å¸ƒå¼è®­ç»ƒä»¥åŠæˆ‘æ­£åœ¨å­¦ä¹ çš„åˆ†å¸ƒå¼æ¨ç†ã€‚å°±è®­ç»ƒè€Œè¨€ï¼Œè‡³å°‘æœ‰ä¸¤ä¸ªæ˜¾è€Œæ˜“è§çš„ç±»åˆ«ï¼š

1. **æ•°æ®å¹¶è¡Œï¼ˆData Parallelismï¼‰**ï¼šè¿™æ˜¯åˆ†å¸ƒå¼è®­ç»ƒä¸­æœ€å¸¸è§çš„å½¢å¼ï¼Œé€‚ç”¨äºå¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡ã€‚åœ¨æ¯å¼  GPU ä¸Šèƒ½å¤Ÿå®Œå…¨å®¹çº³æ•´ä¸ªæ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œæ•°æ®å¹¶è¡Œå°†åŒä¸€æ¨¡å‹çš„å‰¯æœ¬åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šï¼Œæ¯ä¸ª GPU è´Ÿè´£å¤„ç†æ•°æ®é›†çš„ä¸åŒéƒ¨åˆ†ï¼Œç„¶åé€šè¿‡ `all_reduce` ç­‰é›†åˆé€šä¿¡æ“ä½œæ±‡æ€»æ¢¯åº¦å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
- **ä¼˜ç‚¹**ï¼šæ˜“äºå®ç°ï¼Œå°¤å…¶åœ¨å›¾åƒåˆ†ç±»å’Œ NLP ç­‰é¢†åŸŸå¯ä»¥ç›´æ¥åº”ç”¨ã€‚
- **å®ç°æ–¹æ³•**ï¼šé€šè¿‡ `torch.distributed` çš„ `init_process_group()`ã€`all_reduce()` ç­‰å‡½æ•°ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿åœ°åŒæ­¥æ¯ä¸ªè¿›ç¨‹çš„æ¢¯åº¦ï¼Œå®ç°æ•°æ®å¹¶è¡Œã€‚

2. **æ¨¡å‹å¹¶è¡Œï¼ˆModel Parallelismï¼‰**ï¼šåœ¨æ¨¡å‹è§„æ¨¡æå¤§çš„æƒ…å†µä¸‹ï¼Œå•ä¸ªè®¾å¤‡çš„æ˜¾å­˜ä¸è¶³ä»¥å­˜æ”¾æ¨¡å‹å‚æ•°ï¼Œè¿™æ—¶å¯ä»¥å°†æ¨¡å‹æ‹†åˆ†ä¸ºä¸åŒçš„éƒ¨åˆ†ï¼Œç”±å¤šä¸ª GPU å„è‡ªè´Ÿè´£æ¨¡å‹çš„ä¸åŒéƒ¨åˆ†ã€‚
- **ä¼˜ç‚¹**ï¼šå¯ä»¥è®­ç»ƒæ˜¾å­˜è¶…å‡ºå• GPU è´Ÿè·çš„å¤§æ¨¡å‹ã€‚
- **å®ç°æ–¹æ³•**ï¼š`torch.distributed` é€šè¿‡ `send()`ã€`recv()` ç­‰ç‚¹å¯¹ç‚¹é€šä¿¡å‡½æ•°å®ç°æ¨¡å‹ä¸åŒæ¨¡å—ä¹‹é—´çš„æ•°æ®äº¤æ¢ï¼Œä»è€Œå®ç°æ¨¡å‹å¹¶è¡Œã€‚

## è¿›ç¨‹ç»„

åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿä¸­ï¼Œè¿›ç¨‹ç»„æ˜¯ä¸€ä¸ªæ ¸å¿ƒçš„é€šä¿¡å•å…ƒã€‚è¿›ç¨‹ç»„å°†ä¸€ç»„å·²å­˜åœ¨çš„è¿›ç¨‹ç»„ç»‡åœ¨ä¸€èµ·ï¼Œä½¿è¿™äº›è¿›ç¨‹ä¹‹é—´å¯ä»¥é€šè¿‡ç‰¹å®šçš„é€šä¿¡æ–¹å¼è¿›è¡Œæ•°æ®äº¤æ¢ã€‚åœ¨æ¯ä¸ªè¿›ç¨‹å¯åŠ¨æ—¶ï¼Œéœ€è¦å…ˆç”¨ `torch.distributed.init_process_group` åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒå¹¶å°†è¿›ç¨‹åŠ å…¥åˆ°é»˜è®¤çš„å…¨å±€è¿›ç¨‹ç»„ WORLD group ä¸­ã€‚ä¹‹å,å¯ä»¥é€šè¿‡ `torch.distributed.new_group` æ¥åˆ›å»ºæ–°çš„è¿›ç¨‹ç»„ï¼Œå°†ç‰¹å®šçš„è¿›ç¨‹ç»„ç»‡åœ¨ä¸€èµ·ã€‚ä¸åŒè¿›ç¨‹ç»„å¯ä»¥ä½¿ç”¨ä¸åŒçš„é€šä¿¡æ–¹å¼ï¼Œè¿™æ ·å¯ä»¥å®ç°æ›´çµæ´»çš„åˆ†å¸ƒå¼ç­–ç•¥ã€‚

## `init_process_group`

åˆ›å»ºå…¨å±€è¿›ç¨‹ç»„å¹¶å°†è¿›ç¨‹åŠ å…¥å…¶ä¸­ã€‚è¿™ä¸ª API çš„åå­—æœ‰ç‚¹è¿·æƒ‘ï¼Œå› ä¸ºæ¯ä¸ªè¿›ç¨‹é‡Œé¢éƒ½ä¼šæ‰§è¡Œä¸€æ¬¡è¿™ä¸ªæŒ‡ä»¤ï¼Œå¬ä¸Šå»åƒæ˜¯å¯åŠ¨äº† 8 ä¸ªå…¨å±€é»˜è®¤è¿›ç¨‹ç»„ï¼Œå®é™…ä¸Šè¿™é‡Œåšçš„äº‹æƒ…æ˜¯ç±»ä¼¼äº touch æŒ‡ä»¤ã€‚**ç¬¬ä¸€ä¸ªæ‰§è¡Œåˆ°è¿™é‡Œçš„è¿›ç¨‹åˆ›å»ºå¹¶åŠ å…¥å…¨å±€è¿›ç¨‹ç»„ï¼Œä¹‹åæ‰§è¡Œåˆ°çš„è¿›ç¨‹åªéœ€åŠ å…¥ã€‚**

<details>
<summary>init_process_group</summary>

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp


def init_process(rank, world_size):
    print(f"è¿›ç¨‹å·²å¯åŠ¨: æ­¤è¿›ç¨‹çš„ rank æ˜¯ {rank}")
    
    # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„ GPU
    torch.cuda.set_device(rank)
    
    try:
        # åŠ å…¥è¿›ç¨‹ç»„
        print(f"è¿›ç¨‹ {rank} æ­£åœ¨åŠ å…¥è¿›ç¨‹ç»„...")
        dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
        print(f"è¿›ç¨‹ {rank} å·²æˆåŠŸåŠ å…¥è¿›ç¨‹ç»„")
        
        # éªŒè¯èº«ä»½
        assert rank == dist.get_rank()
        assert world_size == dist.get_world_size()
        
        # å‡†å¤‡å½“å‰è¿›ç¨‹çš„ä¿¡æ¯
        process_info = (
            f"\nè¿›ç¨‹ {rank} ä¿¡æ¯:\n"
            f"- Device: {torch.cuda.current_device()}\n"
            f"- GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\n"
        )
        
        # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„å¼ é‡
        max_len = 100  # ç¡®ä¿è¶³å¤Ÿé•¿ä»¥å®¹çº³ä¿¡æ¯
        process_info_tensor = torch.zeros(max_len, dtype=torch.int32, device='cuda')
        process_info_bytes = process_info.encode('utf-8')
        process_info_tensor[:len(process_info_bytes)] = torch.tensor([b for b in process_info_bytes], dtype=torch.int32)
        
        # åˆ›å»ºç”¨äºæ”¶é›†æ‰€æœ‰è¿›ç¨‹ä¿¡æ¯çš„å¼ é‡åˆ—è¡¨
        gathered_tensors = [torch.zeros(max_len, dtype=torch.int32, device='cuda') for _ in range(world_size)]

        # ä½¿ç”¨ all_gather æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„ä¿¡æ¯
        dist.all_gather(gathered_tensors, process_info_tensor)


        if rank == 0:
            print("=============== æ‰€æœ‰è¿›ç¨‹ä¿¡æ¯ ===============")
            for tensor in gathered_tensors:
                info_bytes = tensor.cpu().numpy().astype('uint8').tobytes() 
                info_str = info_bytes.decode('utf-8', 'ignore').strip('\x00')
                print(info_str)
        
        # åˆ›å»ºå¼ é‡å¹¶è¿›è¡Œé€šä¿¡
        tensor = torch.ones(1).cuda() * rank
        print(f"è¿›ç¨‹ {rank} çš„åŸå§‹å¼ é‡å€¼: {tensor.item()}")
        
        # æ‰€æœ‰è¿›ç¨‹åŒæ­¥ç‚¹
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
        print(f"è¿›ç¨‹ {rank} çš„æœ€ç»ˆå¼ é‡å€¼: {tensor.item()}")
    
    finally:
        dist.destroy_process_group()

def main():
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    
    world_size = torch.cuda.device_count()
    print(f"å‡†å¤‡å¯åŠ¨ {world_size} ä¸ªè¿›ç¨‹...")
    
    mp.spawn(
        init_process,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )
    
    #! ç­‰ä»·äºé€šè¿‡ä»¥ä¸‹ä»£ç å¯åŠ¨è¿›ç¨‹
    # processes = []
    # for rank in range(world_size):
    #     p = mp.Process(target=init_process, args=(rank, world_size))
    #     p.start()
    #     processes.append(p)

    # # ç›¸å½“äº join=True çš„æ•ˆæœ
    # for p in processes:
    #     p.join()

if __name__ == "__main__":
    main()
```

</details> 

è¿™æ®µä»£ç çš„æ ¸å¿ƒæ˜¯è¿™ä¸‰ä¸ªæ¥å£ï¼š

1. å°†è¿›ç¨‹åŠ å…¥å…¨å±€è¿›ç¨‹ç»„

`dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)`


2. ç”¨ `all_gather` æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„è®¾å¤‡ä¿¡æ¯

`dist.all_gather(gathered_tensors, process_info_tensor)`

æ¯ä¸ªè¿›ç¨‹å°†è‡ªå·±çš„ä¿¡æ¯å‘é€ç»™å…¶ä»–æ‰€æœ‰è¿›ç¨‹

3. ç”¨ `all_reduce` å¯¹å¼ é‡æ±‚å’Œ

`dist.all_reduce(tensor, op=dist.ReduceOp.SUM)`


## `new_group`

 åˆ›å»ºè‡ªå®šä¹‰è¿›ç¨‹ç»„ï¼Œå’Œ `init_process_group()` ä¸€æ ·ï¼Œåˆ›å»º or åŠ å…¥ã€‚


<details>
<summary>new_group</summary>

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp


def init_process(rank, world_size):
    try:
        # 1. åŠ å…¥å…¨å±€è¿›ç¨‹ç»„
        torch.cuda.set_device(rank)
        if rank == 0:
            print(f"å‡†å¤‡åŠ å…¥å…¨å±€è¿›ç¨‹ç»„...")
        dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
        
        # 2. åˆ›å»ºä¸¤ä¸ªè‡ªå®šä¹‰è¿›ç¨‹ç»„
        group1_ranks = list(range(world_size // 2))
        group2_ranks = list(range(world_size // 2, world_size))
        
        # åˆå§‹åŒ–ç´¯åŠ å€¼ä¸º 0
        group1_sum = torch.zeros(1).cuda()
        group2_sum = torch.zeros(1).cuda()
        if rank == 0:
            print(f"ç»„1çš„åˆå§‹åŒ–ç´¯åŠ å€¼: {group1_sum.item()}")
            print(f"ç»„2çš„åˆå§‹åŒ–ç´¯åŠ å€¼: {group2_sum.item()}")
        
        group1 = dist.new_group(group1_ranks)
        group2 = dist.new_group(group2_ranks)
        
        # 3. åœ¨å„è‡ªçš„ç»„å†…è¿›è¡Œé€šä¿¡
        tensor = torch.ones(1).cuda() * rank  # æ¯ä¸ªè¿›ç¨‹çš„è¾“å…¥å€¼ä¸ºå…¶ rank
        if rank == 0:
            print(f"\nå¼€å§‹è¿›è¡Œç»„å†…é€šä¿¡...")
        
        if rank == 0:
            print(f"Group1 è¿›è¡Œall_reduceæ“ä½œ...")

        # åœ¨å¯¹åº”çš„ç»„å†…è¿›è¡Œall_reduceï¼Œç´¯åŠ ç»“æœä¼šæ›´æ–°åˆ° tensor ä¸­
        if rank in group1_ranks:
            dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group1)
            group1_sum = tensor.clone()  # ä¿å­˜ group1 çš„ç´¯åŠ ç»“æœ
        else:
            dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group2)
            group2_sum = tensor.clone()  # ä¿å­˜ group2 çš„ç´¯åŠ ç»“æœ
        
        # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½èƒ½è·å¾—ä¸¤ä¸ªç»„çš„ç´¯åŠ ç»“æœ
        dist.all_reduce(group1_sum, op=dist.ReduceOp.MAX)
        dist.all_reduce(group2_sum, op=dist.ReduceOp.MAX)
        
        if rank == 0:
            print("\n=============== é€šä¿¡å®Œæˆ ===============")
            print(f"Group1 (ranks {group1_ranks}): ç´¯åŠ ç»“æœä¸º {group1_sum.item()}")
            print(f"Group2 (ranks {group2_ranks}): ç´¯åŠ ç»“æœä¸º {group2_sum.item()}")
    
    finally:
        dist.destroy_process_group()

def main():
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    
    world_size = torch.cuda.device_count()
    print(f"å‡†å¤‡å¯åŠ¨ {world_size} ä¸ªè¿›ç¨‹...")
    
    mp.spawn(
        init_process,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )

if __name__ == "__main__":
    main()
```

</details>


è¿™äº›ä»£ç éƒ½æŒºç®€å•çš„ï¼Œæ¯”è¾ƒæœ‰æ„æ€çš„æ˜¯ï¼Œrank 0 çš„ä»£ç ç»è¿‡ `dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group1)` åï¼Œå°±å·²ç»ä¿ç•™äº†ç¬¬ä¸€ç»„çš„ç´¯åŠ ç»“æœï¼Œä½†æ˜¯è¿™ä¸¤è¡Œä»£ç ä»ç„¶æ˜¯éœ€è¦çš„ï¼š

```python
# ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½èƒ½è·å¾—ä¸¤ä¸ªç»„çš„ç´¯åŠ ç»“æœ
dist.all_reduce(group1_sum, op=dist.ReduceOp.MAX)
dist.all_reduce(group2_sum, op=dist.ReduceOp.MAX)
```

å› ä¸ºï¼Œrank 0 åœ¨ group 1 é‡Œé¢ï¼Œå› æ­¤ `all_reduce(group1_sum)` å¹¶ä¸”å–æœ€å¤§å€¼å¯¹ group1_sum æ²¡å½±å“ã€‚ä½†æ˜¯ rank 0 çš„ group2_sum è¿˜æ˜¯ 0ï¼Œéœ€è¦è¿™æ ·ä¸€ä¸ª all_reduce æ¥å—å…¶ä»– rank çš„ group2_sumã€‚åŸºäºæ­¤ï¼Œè®¾æƒ³ä¸‹ï¼Œç®€å•æŠŠ `dist.ReduceOp.MAX` æ”¹ä¸º `dist.ReduceOp.SUM`ï¼Œç»“æœå°†æ˜¯å…ˆå‰çš„ 4 å€ã€‚

# é€šè®¯æ¥å£

è¿›ç¨‹é—´æ˜¾è€Œæ˜“è§éœ€è¦é€šè®¯ï¼Œæ¯”è¾ƒæœ‰è¶£çš„æ˜¯ï¼Œç®€å•çš„ data parallelism éœ€è¦å¤æ‚çš„ `all_reduce, all_gather, broadcast`ï¼Œè€Œå¤æ‚äº›çš„ model parallelism éœ€è¦ç›´è§‰ä¸Šæ›´ç®€å•çš„ `send, recv`ã€‚å¯¹è¿™äº›é€šè®¯æ–¹å¼åšä¸€è‰è‰åˆ†ç±»ï¼š

1. **ç‚¹å¯¹ç‚¹é€šä¿¡ï¼ˆPoint-to-Point Communicationï¼‰**

ç‚¹å¯¹ç‚¹é€šä¿¡æ˜¯æœ€åŸºç¡€çš„é€šä¿¡æ¨¡å¼ï¼ŒæŒ‡çš„æ˜¯ä¸€ä¸ªè¿›ç¨‹ç›´æ¥å‘å¦ä¸€ä¸ªç‰¹å®šçš„è¿›ç¨‹å‘é€æˆ–æ¥æ”¶æ•°æ®ã€‚è¿™ç§æ¨¡å¼éå¸¸çµæ´»ï¼Œé€‚åˆéœ€è¦ç²¾ç¡®æ§åˆ¶é€šä¿¡è¿‡ç¨‹çš„åœºæ™¯ã€‚

- **send-receive æ¨¡å¼**ï¼šåœ¨ `torch.distributed` ä¸­ï¼Œè¿™ç§æ¨¡å¼å¯ä»¥é€šè¿‡ `send()` å’Œ `recv()` æ¥å£å®ç°ã€‚æ¯”å¦‚ `send(tensor, dst=1)` è¡¨ç¤ºè¿›ç¨‹å°†æ•°æ®å‘é€ç»™ rank ä¸º 1 çš„è¿›ç¨‹ï¼Œè€Œ `recv(tensor, src=0)` è¡¨ç¤ºæ¥æ”¶æ¥è‡ª rank ä¸º 0 çš„è¿›ç¨‹çš„æ•°æ®ã€‚æ¯«æ— ç–‘é—®ï¼Œè¿™æ˜¯é˜»å¡å¼çš„ã€‚

ç‚¹å¯¹ç‚¹é€šä¿¡çš„ä¼˜ç‚¹æ˜¯ç®€å•ç›´è§‚ï¼Œæ˜“äºç†è§£å’Œæ§åˆ¶ï¼›ç¼ºç‚¹æ˜¯å®¹æ˜“å¯¼è‡´å¤æ‚çš„ä»£ç ç»“æ„ï¼Œå°¤å…¶åœ¨éœ€è¦å¤šè¿›ç¨‹ç›¸äº’å‘é€æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šå‡ºç°æ­»é”æˆ–é˜»å¡é—®é¢˜ã€‚å› æ­¤ï¼Œè¿™ç§æ–¹å¼æ›´å¤šé€‚ç”¨äºä¸¤ä¸ªè¿›ç¨‹ä¹‹é—´çš„ä¿¡æ¯äº¤æ¢ã€‚é€‚åˆéœ€è¦ç²¾ç¡®æ§åˆ¶å•ä¸ªè¿›ç¨‹ä¹‹é—´æ•°æ®äº¤æ¢çš„åœºæ™¯ï¼Œé€šå¸¸åœ¨ç³»ç»Ÿå±‚é€šä¿¡ä¼˜åŒ–ä¸­æˆ–æ¨¡å‹åˆ†ç‰‡æ—¶ä½¿ç”¨è¾ƒå¤šã€‚ä¾‹å¦‚åœ¨æ¨¡å‹å¹¶è¡Œè®­ç»ƒçš„æ¢¯åº¦æ›´æ–°ä¸­ï¼Œç‚¹å¯¹ç‚¹é€šä¿¡å¯ä»¥ç”¨äºæ¢¯åº¦çš„æ±‡æ€»ã€‚

2. **é›†åˆé€šä¿¡ï¼ˆCollective Communicationï¼‰**

é›†åˆé€šä¿¡æ˜¯ä¸€ç±»é«˜çº§é€šä¿¡æ¨¡å¼ï¼Œé€šå¸¸ç”¨äºå¤šä¸ªè¿›ç¨‹ä¹‹é—´çš„æ•°æ®äº¤æ¢ã€‚é›†åˆé€šä¿¡æ“ä½œå¾€å¾€ä¼šæ¶‰åŠåˆ°æ‰€æœ‰å‚ä¸çš„è¿›ç¨‹ï¼Œå› æ­¤åœ¨åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ ä¸­ä½¿ç”¨é¢‘ç‡éå¸¸é«˜ã€‚

- **å¹¿æ’­ï¼ˆBroadcastï¼‰**ï¼šå¹¿æ’­æ˜¯ä¸€ç§å°†æ•°æ®ä»ä¸€ä¸ªæºè¿›ç¨‹å‘é€åˆ°æ‰€æœ‰å…¶ä»–è¿›ç¨‹çš„é€šä¿¡æ“ä½œã€‚åœ¨ `torch.distributed` ä¸­ï¼Œé€šè¿‡ `broadcast(tensor, src=0)` å¯ä»¥å®ç°è¯¥æ“ä½œï¼Œå°† rank ä¸º 0 çš„è¿›ç¨‹ä¸­çš„æ•°æ®å¹¿æ’­åˆ°æ‰€æœ‰å…¶ä»–è¿›ç¨‹ã€‚å¹¿æ’­æ“ä½œèƒ½å¤Ÿç¡®ä¿æ‰€æœ‰è¿›ç¨‹æ‹¥æœ‰ç›¸åŒçš„æ•°æ®ï¼Œé€‚åˆéœ€è¦å…±äº«æ¨¡å‹å‚æ•°ã€åˆå§‹åŒ–æƒé‡ç­‰åœºæ™¯ã€‚æ¯”å¦‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒçš„åˆå§‹åŒ–é˜¶æ®µï¼Œç”¨äºå°†ä¸»è¿›ç¨‹çš„æ¨¡å‹å‚æ•°å¹¿æ’­åˆ°æ‰€æœ‰å…¶ä»–è¿›ç¨‹ï¼Œä¿è¯è®­ç»ƒä»åŒæ ·çš„åˆå§‹å‚æ•°å¼€å§‹ã€‚
- **è§„çº¦ï¼ˆReduce å’Œ All-Reduceï¼‰**ï¼šè§„çº¦æ“ä½œæ˜¯ä¸€ç§å°†å¤šä¸ªè¿›ç¨‹çš„æ•°æ®è¿›è¡Œè®¡ç®—ï¼ˆå¦‚æ±‚å’Œã€æ±‚æœ€å¤§å€¼ç­‰ï¼‰çš„æ“ä½œã€‚å¸¸ç”¨çš„è§„çº¦æ“ä½œæœ‰ä¸¤ç§ï¼Œ`reduce()`ï¼šä¸€ä¸ªè¿›ç¨‹ï¼ˆé€šå¸¸æ˜¯ä¸»è¿›ç¨‹ï¼‰æ”¶é›†å¹¶åˆå¹¶æ¥è‡ªæ‰€æœ‰è¿›ç¨‹çš„æ•°æ®ï¼›`all_reduce()`ï¼šæ‰€æœ‰è¿›ç¨‹åŒæ—¶å¾—åˆ°åˆå¹¶åçš„æ•°æ®ã€‚æ¯”å¦‚ `all_reduce(tensor, op=ReduceOp.SUM)` ä¼šåœ¨æ‰€æœ‰è¿›ç¨‹ä¸­æ±‚å’Œï¼Œå¹¶å°†ç»“æœå­˜æ”¾åœ¨æ¯ä¸ªè¿›ç¨‹çš„ `tensor` ä¸­ã€‚è§„çº¦æ“ä½œèƒ½æœ‰æ•ˆå‡å°‘é€šä¿¡è´Ÿæ‹…ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ¢¯åº¦æ±‡æ€»æˆ–æ¨¡å‹æƒé‡æ›´æ–°ã€‚è­¬å¦‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œ`all_reduce` å¸¸ç”¨äºæ¢¯åº¦æ±‚å’Œï¼Œä»¥ç¡®ä¿åœ¨å¤šä¸ªè¿›ç¨‹ä¸­çš„æ¢¯åº¦ä¿æŒä¸€è‡´ï¼Œå®ç°åŒæ­¥æ›´æ–°ã€‚
- **æ”¶é›†ï¼ˆGather å’Œ All-Gatherï¼‰**ï¼šæ”¶é›†æ“ä½œæ˜¯å°†å¤šä¸ªè¿›ç¨‹çš„æ•°æ®æ”¶é›†åˆ°ä¸€ä¸ªæˆ–å¤šä¸ªè¿›ç¨‹çš„æ“ä½œï¼š`gather()`ï¼šå°†å¤šä¸ªè¿›ç¨‹çš„æ•°æ®æ”¶é›†åˆ°ä¸€ä¸ªè¿›ç¨‹ä¸­ã€‚`all_gather()`ï¼šæ‰€æœ‰è¿›ç¨‹éƒ½æ”¶é›†åˆ°å…¨éƒ¨è¿›ç¨‹çš„æ•°æ®ã€‚ä¾‹å¦‚ `all_gather(gathered_tensors, tensor)` ä¼šå°†æ‰€æœ‰è¿›ç¨‹ä¸­çš„ `tensor` æ”¶é›†åˆ°æ¯ä¸ªè¿›ç¨‹çš„ `gathered_tensors` åˆ—è¡¨ä¸­ã€‚æ”¶é›†æ“ä½œæ–¹ä¾¿å¯¹æ‰€æœ‰è¿›ç¨‹ä¸­çš„æ•°æ®è¿›è¡Œåç»­åˆ†æå’Œå¤„ç†ã€‚è­¬å¦‚åš evaluation æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ `all_gather` æ¥æ±‡æ€»å„ä¸ªè¿›ç¨‹çš„ä¸­é—´ç»“æœã€‚
- **æ•£å‘ï¼ˆScatterï¼‰**ï¼š`scatter()` æ“ä½œæ˜¯å°†ä¸€ä¸ªè¿›ç¨‹çš„æ•°æ®åˆ†æ•£åˆ°å¤šä¸ªè¿›ç¨‹ä¸­ã€‚ä¾‹å¦‚åœ¨ rank ä¸º 0 çš„è¿›ç¨‹ä¸­æœ‰ä¸€ä¸ªåŒ…å«è‹¥å¹²å­å¼ é‡çš„åˆ—è¡¨ï¼Œ`scatter()` å¯ä»¥å°†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå­å¼ é‡åˆ†é…ç»™å…¶ä»–è¿›ç¨‹ã€‚é€‚ç”¨äºæ•°æ®åˆ†å‘ï¼Œå°†å¤§å‹æ•°æ®é›†æˆ–æ¨¡å‹æƒé‡åœ¨å¤šä¸ªè¿›ç¨‹ä¸­åˆ†æ•£ï¼Œä»¥ä¾¿æ¯ä¸ªè¿›ç¨‹å¯ä»¥å¤„ç†ä¸åŒçš„æ•°æ®å—ã€‚

3. **ç‚¹å¯¹ç‚¹å’Œé›†åˆé€šè®¯å¯¹æ¯”**

- **çµæ´»æ€§**ï¼šç‚¹å¯¹ç‚¹é€šä¿¡é€‚åˆéœ€è¦é«˜ç²¾åº¦æ§åˆ¶é€šä¿¡çš„åœºæ™¯ï¼Œä½†ä¸é€‚åˆå¤§è§„æ¨¡é€šä¿¡ï¼Œå› ä¸ºä»£ç ä¼šå˜å¾—å¤æ‚ã€‚é›†åˆé€šä¿¡æ›´é«˜æ•ˆï¼Œé€‚åˆå¤šè¿›ç¨‹åä½œåœºæ™¯ï¼Œå°¤å…¶åœ¨æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­ã€‚
- **å¤æ‚åº¦**ï¼šé›†åˆé€šä¿¡ç®€åŒ–äº†æ•°æ®åŒæ­¥ã€æ¢¯åº¦è§„çº¦ç­‰å¸¸è§éœ€æ±‚ï¼Œå¹¶èƒ½æé«˜è®­ç»ƒçš„é€Ÿåº¦å’Œé€šä¿¡æ•ˆç‡ã€‚

## `send` and `recv`


<details>
<summary>send and recv</summary>

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

def init_process(rank, world_size):
    try:
        torch.cuda.set_device(rank)
        dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
        
        # åˆ›å»ºåˆå§‹æ•°æ®ï¼ˆåªåœ¨ rank 0 åˆ›å»ºæœ‰æ„ä¹‰çš„æ•°æ®ï¼‰
        if rank == 0:
            tensor = torch.tensor([100, 200], dtype=torch.float32).cuda()
            print(f"\n=== åˆå§‹çŠ¶æ€ ===")
            print(f"Rank 0 çš„åˆå§‹æ•°æ®: {tensor}")
            # å‘é€æ•°æ®ç»™ rank 1
            dist.send(tensor, dst=1)
            print(f"Rank 0 å·²å‘é€æ•°æ®åˆ° Rank 1")
            
        elif rank == 1:
            # rank 1 æ¥æ”¶æ¥è‡ª rank 0 çš„æ•°æ®
            tensor = torch.zeros(2, dtype=torch.float32).cuda()
            dist.recv(tensor, src=0)
            print(f"Rank 1 æ”¶åˆ°æ•°æ®: {tensor}")
            
            # å¯¹æ•°æ®è¿›è¡Œä¿®æ”¹åå‘é€ç»™ rank 2
            tensor = tensor * 2  # å°†æ•°æ®ç¿»å€
            print(f"Rank 1 å¤„ç†åçš„æ•°æ®: {tensor}")
            dist.send(tensor, dst=2)
            print(f"Rank 1 å·²å‘é€æ•°æ®åˆ° Rank 2")
            
        elif rank == 2:
            # rank 2 æ¥æ”¶æ¥è‡ª rank 1 çš„æ•°æ®
            tensor = torch.zeros(2, dtype=torch.float32).cuda()
            dist.recv(tensor, src=1)
            print(f"Rank 2 æ”¶åˆ°æ•°æ®: {tensor}")
            print("\n=== ä¼ è¾“å®Œæˆ ===")
            
    finally:
        dist.destroy_process_group()

def main():
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    
    world_size = 3
    print(f"å‡†å¤‡å¯åŠ¨ {world_size} ä¸ªè¿›ç¨‹...")
    
    mp.spawn(
        init_process,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )

if __name__ == "__main__":
    main()
```

</details>
ç”¨æ³•éå¸¸ç®€å•ã€‚

- `recv` éœ€è¦é¢„å…ˆåˆ†é…å¥½æ¥æ”¶æ•°æ®çš„ tensorï¼Œä¸”å¤§å°å¿…é¡»åŒ¹é…ã€‚
- `send` å’Œ `recv` éƒ½æ˜¯é˜»å¡æ“ä½œï¼Œå‘é€æ–¹ä¼šç­‰å¾…ç›´åˆ°æ¥æ”¶æ–¹å®Œæˆæ¥æ”¶ï¼Œæ¥æ”¶æ–¹ä¼šç­‰å¾…ç›´åˆ°å‘é€æ–¹çš„æ•°æ®åˆ°è¾¾ã€‚
- æ¯ä¸ª `send` å¿…é¡»æœ‰å¯¹åº”çš„ `recv`ï¼Œå¦‚æœé…å¯¹ä¸å½“ä¼šå¯¼è‡´æ­»é”ã€‚

ä¸¾ä¸ªä½¿ç”¨ä¸å½“çš„ä¾‹å­ï¼š

```python
# é”™è¯¯ç¤ºä¾‹ - å¯èƒ½å¯¼è‡´æ­»é”
if rank == 0:
    dist.send(tensor1, dst=1)  # ç­‰å¾… rank 1 æ¥æ”¶
    dist.recv(tensor2, src=1)  # æ°¸è¿œç­‰ä¸åˆ°ï¼Œå› ä¸º rank 1 å¡åœ¨å‘é€
elif rank == 1:
    dist.send(tensor2, dst=0)  # ç­‰å¾… rank 0 æ¥æ”¶
    dist.recv(tensor1, src=0)  # æ°¸è¿œç­‰ä¸åˆ°ï¼Œå› ä¸º rank 0 å¡åœ¨å‘é€

# æ­£ç¡®ç¤ºä¾‹
if rank == 0:
    dist.send(tensor1, dst=1)
    dist.recv(tensor2, src=1)
elif rank == 1:
    dist.recv(tensor1, src=0)  # å…ˆæ¥æ”¶
    dist.send(tensor2, dst=0)  # å†å‘é€
```

- å‘é€å’Œæ¥æ”¶çš„ tensor å¿…é¡»åœ¨ç›¸åŒç±»å‹çš„è®¾å¤‡ä¸Šï¼ˆéƒ½åœ¨ CPU æˆ–éƒ½åœ¨ GPUï¼‰ã€‚

- å¯¹äºç®€å•çš„é›†åˆé€šä¿¡ï¼Œå»ºè®®ä½¿ç”¨ä¸“é—¨çš„é›†åˆé€šä¿¡åŸè¯­ï¼š`all_reduce` ä»£æ›¿å¤šä¸ª `send/recv` çš„æ±‚å’Œï¼Œ`all_gather` ä»£æ›¿å¤šä¸ª `send/recv` çš„æ•°æ®æ”¶é›†ï¼Œ`broadcast` ä»£æ›¿ä¸€å¯¹å¤šçš„å‘é€ã€‚

## `isend` and `irecv`

- å¦‚æœéœ€è¦éé˜»å¡é€šä¿¡ï¼Œå¯ä»¥ä½¿ç”¨ `isend/irecv`
- ä¹Ÿå¯ä»¥ä½¿ç”¨[dist.batch_isend_irecv](https://pytorch.org/docs/stable/distributed.html#torch.distributed.batch_isend_irecv) fuseå¤šä¸ªP2Pé€šä¿¡æ“ä½œ. è¯¥å‡½æ•°ä¼šå°è¯•[fuseå¤šä¸ªNCCL kernelæ¥æé«˜throughput](https://github.com/pytorch/pytorch/issues/132852)ï¼Œå¹¶re-orderé€šä¿¡é¡ºåºä»¥å‡å°‘deadlockæ¦‚ç‡ã€‚

<details>
<summary>isend and irecv</summary>

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import time  # æ·»åŠ  time ç”¨äºæ¼”ç¤ºå¼‚æ­¥æ•ˆæœ

def init_process(rank, world_size):
    try:
        torch.cuda.set_device(rank)
        dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
        
        if rank == 0:
            tensor = torch.tensor([100, 200], dtype=torch.float32).cuda()
            print(f"\n=== åˆå§‹çŠ¶æ€ ===")
            print(f"Rank 0 çš„åˆå§‹æ•°æ®: {tensor}")
            
            # å¼‚æ­¥å‘é€æ•°æ®ç»™ rank 1
            print(f"Rank 0 å‡†å¤‡å‘é€æ•°æ®...")
            send_req = dist.isend(tensor, dst=1)
            print(f"Rank 0 å¯åŠ¨å¼‚æ­¥å‘é€")
            
            # æ¨¡æ‹Ÿåœ¨ç­‰å¾…å‘é€å®Œæˆæ—¶åšå…¶ä»–å·¥ä½œ
            print(f"Rank 0 æ­£åœ¨å¤„ç†å…¶ä»–ä»»åŠ¡...")
            time.sleep(1)  # æ¨¡æ‹Ÿå…¶ä»–è®¡ç®—ä»»åŠ¡
            
            # ç­‰å¾…å‘é€å®Œæˆ
            send_req.wait()
            print(f"Rank 0 ç¡®è®¤å‘é€å®Œæˆ")
            
        elif rank == 1:
            tensor = torch.zeros(2, dtype=torch.float32).cuda()
            print(f"Rank 1 å‡†å¤‡æ¥æ”¶æ•°æ®...")
            
            # å¼‚æ­¥æ¥æ”¶æ¥è‡ª rank 0 çš„æ•°æ®
            recv_req = dist.irecv(tensor, src=0)
            print(f"Rank 1 å¯åŠ¨å¼‚æ­¥æ¥æ”¶")
            
            # æ¨¡æ‹Ÿåœ¨ç­‰å¾…æ¥æ”¶å®Œæˆæ—¶åšå…¶ä»–å·¥ä½œ
            print(f"Rank 1 æ­£åœ¨å¤„ç†å…¶ä»–ä»»åŠ¡...")
            time.sleep(1)  # æ¨¡æ‹Ÿå…¶ä»–è®¡ç®—ä»»åŠ¡
            
            # ç­‰å¾…æ¥æ”¶å®Œæˆ
            recv_req.wait()
            print(f"Rank 1 æ¥æ”¶å®Œæˆï¼Œæ•°æ®ä¸º: {tensor}")
            
            # å¤„ç†æ•°æ®å¹¶å¼‚æ­¥å‘é€ç»™ rank 2
            tensor = tensor * 2
            print(f"Rank 1 å¤„ç†åçš„æ•°æ®: {tensor}")
            print(f"Rank 1 å‡†å¤‡å‘é€æ•°æ®ç»™ Rank 2...")
            send_req = dist.isend(tensor, dst=2)
            print(f"Rank 1 å¯åŠ¨å¼‚æ­¥å‘é€")
            
            # æ¨¡æ‹Ÿåœ¨ç­‰å¾…å‘é€å®Œæˆæ—¶åšå…¶ä»–å·¥ä½œ
            print(f"Rank 1 æ­£åœ¨å¤„ç†å…¶ä»–ä»»åŠ¡...")
            time.sleep(1)  # æ¨¡æ‹Ÿå…¶ä»–è®¡ç®—ä»»åŠ¡
            
            send_req.wait()
            print(f"Rank 1 ç¡®è®¤å‘é€å®Œæˆ")
            
        elif rank == 2:
            tensor = torch.zeros(2, dtype=torch.float32).cuda()
            print(f"Rank 2 å‡†å¤‡æ¥æ”¶æ•°æ®...")
            
            # å¼‚æ­¥æ¥æ”¶æ¥è‡ª rank 1 çš„æ•°æ®
            recv_req = dist.irecv(tensor, src=1)
            print(f"Rank 2 å¯åŠ¨å¼‚æ­¥æ¥æ”¶")
            
            # æ¨¡æ‹Ÿåœ¨ç­‰å¾…æ¥æ”¶å®Œæˆæ—¶åšå…¶ä»–å·¥ä½œ
            print(f"Rank 2 æ­£åœ¨å¤„ç†å…¶ä»–ä»»åŠ¡...")
            time.sleep(1)  # æ¨¡æ‹Ÿå…¶ä»–è®¡ç®—ä»»åŠ¡
            
            # ç­‰å¾…æ¥æ”¶å®Œæˆ
            recv_req.wait()
            print(f"Rank 2 æ¥æ”¶å®Œæˆï¼Œæœ€ç»ˆæ•°æ®ä¸º: {tensor}")
            print("\n=== ä¼ è¾“å®Œæˆ ===")
            
    finally:
        dist.destroy_process_group()

def main():
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    
    world_size = 3
    print(f"å‡†å¤‡å¯åŠ¨ {world_size} ä¸ªè¿›ç¨‹...")
    
    mp.spawn(
        init_process,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )

if __name__ == "__main__":
    main()
```

</details>

- åœ¨é€šä¿¡å®Œæˆå‰ä¸è¦ä¿®æ”¹å‘é€ç¼“å†²åŒº(buffer)ï¼Œåœ¨é€šä¿¡å®Œæˆå‰ä¸è¦ä½¿ç”¨æ¥æ”¶ç¼“å†²åŒºï¼Œå¿…é¡»ç­‰å¾… `wait()` å®Œæˆåæ‰èƒ½å®‰å…¨æ“ä½œç›¸å…³æ•°æ®
- æ¯ä¸ªå¼‚æ­¥æ“ä½œéƒ½ä¼šå ç”¨ç³»ç»Ÿèµ„æºï¼Œåº”åŠæ—¶è°ƒç”¨ `wait()` é‡Šæ”¾èµ„æº
- é¿å…åŒæ—¶å‘èµ·è¿‡å¤šæœªå®Œæˆçš„å¼‚æ­¥æ“ä½œ
- å¼‚æ­¥æ“ä½œå¯èƒ½åœ¨åå°å¤±è´¥ï¼Œ`wait()` è°ƒç”¨ä¼šæš´éœ²é€šä¿¡è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œå»ºè®®ä½¿ç”¨ `try-finally` ç¡®ä¿èµ„æºæ­£ç¡®æ¸…ç†

## `all_reduce` and `all_gather`

1. **åŠŸèƒ½å®šä½**ï¼š
- `all_reduce`: å¯¹æ‰€æœ‰è¿›ç¨‹çš„æ•°æ®è¿›è¡Œè§„çº¦ï¼ˆreductionï¼‰æ“ä½œï¼Œå¦‚æ±‚å’Œã€å–æœ€å¤§å€¼ç­‰
- `all_gather`: æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„æ•°æ®ï¼Œä¸è¿›è¡Œè¿ç®—ï¼Œåªæ˜¯ç®€å•åˆå¹¶

2. **è¾“å‡ºç»“æœ**ï¼š
- `all_reduce`: æ‰€æœ‰è¿›ç¨‹å¾—åˆ°ç›¸åŒçš„è§„çº¦ç»“æœ
- `all_gather`: æ‰€æœ‰è¿›ç¨‹å¾—åˆ°åŒ…å«æ‰€æœ‰è¿›ç¨‹åŸå§‹æ•°æ®çš„å®Œæ•´åˆ—è¡¨

3. **å†…å­˜ä½¿ç”¨**ï¼š
- `all_reduce`: è¾“å‡ºå¼ é‡å¤§å°ä¸è¾“å…¥ç›¸åŒ
- `all_gather`: è¾“å‡ºå¼ é‡å¤§å°æ˜¯è¾“å…¥çš„ `world_size` å€

4. **é€‚ç”¨åœºæ™¯**ï¼š
- `all_reduce`ï¼šè®¡ç®—åˆ†å¸ƒå¼æŸå¤±ï¼Œæ¢¯åº¦åŒæ­¥ï¼Œè®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰
- `all_gather`ï¼šè·å–å…¶ä»–è¿›ç¨‹çš„åŸå§‹æ•°æ®ï¼Œåˆ†å¸ƒå¼è¯„ä¼°æŒ‡æ ‡è®¡ç®—ï¼Œæ±‡æ€»ä¸åŒè¿›ç¨‹çš„ä¸­é—´ç»“æœ

5. **é€šè®¯æ•ˆç‡**ï¼š

- `all_reduce` é€šå¸¸æ¯” `all_gather` æ›´é«˜æ•ˆï¼Œå¦‚æœåªéœ€è¦å¾—åˆ°æœ€ç»ˆçš„æ±‡æ€»ç»“æœï¼Œåº”ä¼˜å…ˆä½¿ç”¨ `all_reduce`ï¼Œä¼ è¾“çš„æ•°æ®é‡æ›´å°ï¼Œå¯ä»¥åˆ©ç”¨æ ‘å½¢ç»“æ„è¿›è¡Œè§„çº¦ã€‚


<details>
<summary>all_reduce and all_gather</summary>

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

def init_process(rank, world_size):
    try:
        torch.cuda.set_device(rank)
        dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
        
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        tensor = torch.tensor([rank * 10, rank * 10 + 1], dtype=torch.float32).cuda()
        
        # === all_gather ç¤ºä¾‹ ===
        gathered = [torch.zeros(2, dtype=torch.float32).cuda() for _ in range(world_size)]
        dist.all_gather(gathered, tensor)
        
        if rank == 0:
            print("\n=== all_gather ç»“æœ ===")
            print(f"åŸå§‹å¼ é‡ (rank 0): {tensor}")
            print("æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„å¼ é‡:")
            for i, t in enumerate(gathered):
                print(f"rank {i} çš„æ•°æ®: {t.tolist()}")
        
        # === all_reduce ç¤ºä¾‹ ===
        reduced_tensor = tensor.clone()  # åˆ›å»ºå‰¯æœ¬ç”¨äº all_reduce
        if rank == 0:
            print(f"before all_reduce: {reduced_tensor}")

        dist.all_reduce(reduced_tensor, op=dist.ReduceOp.SUM)
        
        if rank == 0:
            print("\n=== all_reduce ç»“æœ ===")
            print(f"åŸå§‹å¼ é‡ (rank 0): {tensor}")
            print(f"å½’çº¦åçš„å¼ é‡ (æ‰€æœ‰ rank çš„å’Œ): {reduced_tensor}")
            
    finally:
        dist.destroy_process_group()
        
def main():
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    
    world_size = torch.cuda.device_count()
    print(f"å‡†å¤‡å¯åŠ¨ {world_size} ä¸ªè¿›ç¨‹...")
    
    mp.spawn(
        init_process,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )

if __name__ == "__main__":
    main()
```

</details>

å®é™…ä¸Š `all_reduce` æœ¬èº«åªæ”¯æŒæœ‰é™çš„è¿ç®—ï¼Œå¯ä»¥é€šè¿‡è¿™äº›è¿ç®—çš„ç»„åˆå®ç°å¤æ‚ä¸€äº›å‡½æ•°ï¼Œç±»ä¼¼äºå®ç°åˆ†å¸ƒå¼çš„ `softmax`ã€‚


<details>
<summary>all_reduce å®ç° softmax</summary>

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp


def init_process(rank, world_size):
    try:
        torch.cuda.set_device(rank)
        dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
        
        # åˆ›å»ºæ›´å¤æ‚çš„æµ‹è¯•å¼ é‡
        tensor = torch.tensor([rank + 1, rank + 2, rank + 3], dtype=torch.float32).cuda()
        
        if rank == 0:
            print(f"\nåˆå§‹å¼ é‡ (rank {rank}): {tensor}")
            
        # 1. ä½¿ç”¨ PREMUL_SUM å®ç°åŠ æƒå’Œ
        weights = torch.tensor([0.3, 0.3, 0.4]).cuda()
        weighted = tensor * weights
        dist.all_reduce(weighted, op=dist.ReduceOp.SUM)
        
        if rank == 0:
            print(f"\n=== åŠ æƒå’Œç»“æœ ===")
            print(f"åŠ æƒåçš„å¼ é‡: {weighted}")
            
        # 2. å®ç° softmax çš„åˆ†å¸ƒå¼ç‰ˆæœ¬
        # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—æœ€å¤§å€¼
        max_tensor = tensor.clone()
        if rank == 0:
            print(f"max_tensor before all_reduce: {max_tensor}")
        dist.all_reduce(max_tensor, op=dist.ReduceOp.MAX)
        
        if rank == 0:
            print(f"max_tensor after all_reduce: {max_tensor}")
        
        # ç¬¬äºŒæ­¥ï¼šè®¡ç®— exp(x - max(x))
        exp_tensor = torch.exp(tensor - max_tensor)
        
        # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—åˆ†æ¯ï¼ˆæ‰€æœ‰expçš„å’Œï¼‰
        sum_exp = exp_tensor.clone()
        if rank == 0:
            print(f"sum_exp before all_reduce: {sum_exp}")
        dist.all_reduce(sum_exp, op=dist.ReduceOp.SUM)
        
        if rank == 0:
            print(f"sum_exp after all_reduce: {sum_exp}")
        
        # ç¬¬å››æ­¥ï¼šè®¡ç®—æœ€ç»ˆçš„ softmax
        softmax_result = exp_tensor / sum_exp
        
        if rank == 0:
            print(f"\n=== åˆ†å¸ƒå¼ Softmax ç»“æœ ===")
            print(f"Softmax ç»“æœ: {softmax_result}")
            
        # 3. å®ç° L2 æ­£åˆ™åŒ–çš„åˆ†å¸ƒå¼ç‰ˆæœ¬
        # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—å¹³æ–¹
        squared = tensor ** 2
        
        # ç¬¬äºŒæ­¥ï¼šæ±‚æ‰€æœ‰å…ƒç´ å¹³æ–¹å’Œ
        sum_squared = squared.clone()
        if rank == 0:
            print(f"sum_squared before all_reduce: {sum_squared}")
        dist.all_reduce(sum_squared, op=dist.ReduceOp.SUM)
        
        if rank == 0:
            print(f"sum_squared after all_reduce: {sum_squared}")
        
        # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—å¹³æ–¹æ ¹
        l2_norm = torch.sqrt(sum_squared)
        
        # ç¬¬å››æ­¥ï¼šæ­£åˆ™åŒ–
        normalized = tensor / l2_norm
        
        if rank == 0:
            print(f"\n=== åˆ†å¸ƒå¼ L2 æ­£åˆ™åŒ–ç»“æœ ===")
            print(f"æ­£åˆ™åŒ–ç»“æœ: {normalized}")
            
    finally:
        dist.destroy_process_group()
        
def main():
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    
    world_size = torch.cuda.device_count()
    print(f"å‡†å¤‡å¯åŠ¨ {world_size} ä¸ªè¿›ç¨‹...")
    
    mp.spawn(
        init_process,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )

if __name__ == "__main__":
    main()
```


</details>

## `broadcast`

<details>
<summary>broadcast</summary>

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

def init_process(rank, world_size):
    try:
        # åˆå§‹åŒ–è¿›ç¨‹ç»„
        torch.cuda.set_device(rank)
        dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)

        # åˆ›å»ºæ•°æ®
        if rank == 0:
            data1 = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]).cuda()
            data2 = torch.zeros(3).cuda()  # ç”¨äºæ¥æ”¶rank 1çš„å¹¿æ’­
            print(f"Rank 0 åˆå§‹æ•°æ®: data1={data1}, data2={data2}")
        elif rank == 1:
            data1 = torch.zeros(5).cuda()  # ç”¨äºæ¥æ”¶rank 0çš„å¹¿æ’­
            data2 = torch.tensor([10.0, 20.0, 30.0]).cuda()
            print(f"Rank 1 åˆå§‹æ•°æ®: data1={data1}, data2={data2}")
        else:
            data1 = torch.zeros(5).cuda()
            data2 = torch.zeros(3).cuda()
            print(f"Rank {rank} åˆå§‹æ•°æ®: data1={data1}, data2={data2}")
        
        # å…ˆæ‰§è¡Œrank 0çš„å¹¿æ’­
        dist.broadcast(data1, src=0)
        print(f"Rank {rank} ç¬¬ä¸€æ¬¡å¹¿æ’­å: data1={data1}")
        print(f"Rank {rank} ç¬¬ä¸€æ¬¡å¹¿æ’­å: data2={data2}")
        
        # å†æ‰§è¡Œrank 1çš„å¹¿æ’­
        dist.broadcast(data2, src=1)
        print(f"Rank {rank} ç¬¬äºŒæ¬¡å¹¿æ’­å: data1={data1}")
        print(f"Rank {rank} ç¬¬äºŒæ¬¡å¹¿æ’­å: data2={data2}")

    finally:
        dist.destroy_process_group()

def main():
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    
    world_size = torch.cuda.device_count()
    print(f"å‡†å¤‡å¯åŠ¨ {world_size} ä¸ªè¿›ç¨‹...\n")
    
    mp.spawn(
        init_process,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )

if __name__ == "__main__":
    main()
```


</details>

ä¾‹å­éå¸¸ç®€å•ï¼š

1. `broadcast` å°†æºè¿›ç¨‹ src çš„å¼ é‡æ•°æ®å¹¿æ’­åˆ°æ‰€æœ‰å…¶ä»–è¿›ç¨‹çš„åŒåå¼ é‡
2. æ¥æ”¶æ•°æ®çš„è¿›ç¨‹å¿…é¡»é¢„å…ˆåˆ†é…å¥½ç›¸åŒå¤§å°çš„å¼ é‡ç©ºé—´
3. å¹¿æ’­æ“ä½œæ˜¯é˜»å¡çš„ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½éœ€è¦æ‰§è¡Œåˆ°è¿™è¡Œä»£ç æ‰èƒ½ç»§ç»­
4. æ•°æ®ä¼šç›´æ¥åœ¨é¢„åˆ†é…çš„å†…å­˜ä¸Šè¿›è¡Œä¿®æ”¹ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°çš„å¼ é‡

## `scatter`


<details>
<summary>scatter</summary>

```python
import os
import torch
import torch.distributed as dist
import torch.multiprocessing as mp


def init_process(rank, world_size):
    try:
        torch.cuda.set_device(rank)
        dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
        
        # === scatter ç¤ºä¾‹ ===

        if rank == 0:
            # åœ¨ rank 0 åˆ›å»ºè¦åˆ†å‘çš„æ•°æ®
            # ä¸ºæ¯ä¸ªè¿›ç¨‹å‡†å¤‡ 2 ä¸ªæ•°å­—
            scatter_list = [
                torch.tensor([i * 10, i * 10 + 1], dtype=torch.float32).cuda()
                for i in range(world_size)
            ]
            print("\n=== scatter å‰çš„æ•°æ® ===")
            for i, tensor in enumerate(scatter_list):
                print(f"å‡†å¤‡å‘é€åˆ° rank {i} çš„æ•°æ®: {tensor.tolist()}")
        else:
            scatter_list = None

        # å‡†å¤‡æ¥æ”¶æ•°æ®çš„å¼ é‡
        output_tensor = torch.zeros(2, dtype=torch.float32).cuda()
        print(f"Rank {rank} åˆå§‹åŒ–æ¥æ”¶æ•°æ®: {output_tensor.tolist()}")
        
        # æ‰§è¡Œ scatter æ“ä½œ
        dist.scatter(output_tensor, scatter_list, src=0)
        
        # æ¯ä¸ªè¿›ç¨‹æ‰“å°æ¥æ”¶åˆ°çš„æ•°æ®
        print(f"Rank {rank} æ”¶åˆ°çš„æ•°æ®: {output_tensor.tolist()}")
            
    finally:
        dist.destroy_process_group()
        
def main():
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '29500'
    
    world_size = torch.cuda.device_count()
    print(f"å‡†å¤‡å¯åŠ¨ {world_size} ä¸ªè¿›ç¨‹...")
    
    mp.spawn(
        init_process,
        args=(world_size,),
        nprocs=world_size,
        join=True
    )

if __name__ == "__main__":
    main()
```


</details>

- `scatter` æ˜¯ä¸€å¯¹å¤šçš„åˆ†å‘æ“ä½œï¼Œåªæœ‰æºè¿›ç¨‹(è¿™é‡Œæ˜¯ rank 0)éœ€è¦å‡†å¤‡å®Œæ•´æ•°æ®
- å…¶ä»–è¿›ç¨‹çš„ `scatter_list` å¿…é¡»è®¾ä¸º Noneï¼Œè¿™æ˜¯ PyTorch çš„è§„å®š
- æ•°æ®å¿…é¡»äº‹å…ˆæŒ‰è¿›ç¨‹æ•°é‡åˆ‡åˆ†å¥½ï¼Œæ¯ä¸ªè¿›ç¨‹è·å¾—ä¸€ä»½
- `scatter` æ“ä½œæ˜¯åŒæ­¥çš„ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½ä¼šåœ¨è¿™é‡Œç­‰å¾…ï¼Œç›´åˆ°é€šä¿¡å®Œæˆ
- å¿…é¡»æŒ‡å®šæºè¿›ç¨‹ (src=0)ï¼Œè¡¨æ˜æ•°æ®ä»å“ªä¸ªè¿›ç¨‹åˆ†å‘å‡ºå»
- `scatter_list` ä¸­çš„æ¯ä¸ªå¼ é‡å¤§å°å¿…é¡»ç›¸åŒ
- æ€»æ•°æ®é‡å¿…é¡»èƒ½è¢«è¿›ç¨‹æ•°æ•´é™¤

- `scatter` é€‚åˆå°†å¤§æ•°æ®é›†åˆ’åˆ†ç»™å¤šä¸ªè¿›ç¨‹å¤„ç†

- ç›¸æ¯” `broadcast`ï¼Œ`scatter` å¯ä»¥èŠ‚çœå…¶ä»–è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨

**`scatter` é€‚åˆï¼š**

1. æ•°æ®å¹¶è¡Œè®­ç»ƒæ—¶åˆ†å‘ä¸åŒçš„æ•°æ®æ‰¹æ¬¡
2. å°†å¤§è§„æ¨¡æ•°æ®é›†åˆ†ç‰‡åˆ°å¤šä¸ªèŠ‚ç‚¹è¿›è¡Œå¤„ç†
3. åœ¨å‚æ•°æœåŠ¡å™¨æ¶æ„ä¸­åˆ†å‘æ¨¡å‹å‚æ•°

**ä¸ºä»€ä¹ˆè¯´ `scatter` æ¯”èµ· `broadcast` èŠ‚çœç©ºé—´ï¼Ÿ**

è€ƒè™‘ä¸€å…± 4 ä¸ªè¿›ç¨‹ï¼Œéœ€è¦ä» rank 0 å‘ `[1000, 250]` ç»´åº¦çš„æ•°æ®ç»™ rank 1, 2, 3ï¼Œé‚£ä¹ˆç”¨ `broadcast` åˆ™æ¯å¼ å¡ä¸Šéƒ½å¾—æœ‰ `[1000, 250]` å¤§å°çš„çš„æ•°æ®å—ï¼Œç„¶åå„è‡ªåˆ‡ç‰‡ã€‚ä½¿ç”¨ `scatter` åˆ™åªæœ‰ rank 0 ä¸Šä¼šæœ‰ `[1000, 1000]`ï¼Œå…¶ä»– rank ä¸Šæ˜¯ `[1000, 250]`ã€‚

# åè®°

è¿™é‡Œæ‘˜å½•ä¸€äº›å¯¹æˆ‘è›®æœ‰å¯å‘çš„åšå®¢çš„è®°å½•ã€‚

å‚è€ƒäº†çŸ¥ä¹[[åŸåˆ›][æ·±åº¦][PyTorch] DDPç³»åˆ—ç¬¬ä¸€ç¯‡ï¼šå…¥é—¨æ•™ç¨‹](https://zhuanlan.zhihu.com/p/178402798)ã€‚

æœ‰å‡ ä¸ªéå¸¸é‡è¦çš„æ¦‚å¿µï¼Œæˆ‘ç»§ç»­é—® claude è¡¥å……ä¸‹ï¼š

## GIL

ä¼—æ‰€å‘¨çŸ¥å› ä¸º GIL çš„å­˜åœ¨ï¼ŒPython çš„å¤šçº¿ç¨‹æ˜¯ä¼ªå¤šçº¿ç¨‹ã€‚GILï¼ˆGlobal Interpreter Lockï¼Œå…¨å±€è§£é‡Šå™¨é”ï¼‰æ˜¯ Python è§£é‡Šå™¨ CPython ä¸­çš„ä¸€ä¸ªäº’æ–¥é”ï¼Œå®ƒå¯ä»¥ç¡®ä¿åœ¨ä»»ä½•æ—¶å€™åªæœ‰ä¸€ä¸ªçº¿ç¨‹èƒ½å¤Ÿæ‰§è¡Œ Python å­—èŠ‚ç ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå³ä½¿åœ¨å¤šæ ¸å¤„ç†å™¨ä¸Šï¼Œä¸€ä¸ª Python è¿›ç¨‹åŒä¸€æ—¶åˆ»ä¹Ÿåªèƒ½æ‰§è¡Œä¸€ä¸ªçº¿ç¨‹ã€‚

GIL çš„è®¾è®¡å¯ä»¥è¿½æº¯åˆ° 1992 å¹´å½“æ—¶æ˜¯ä¸ºäº†è§£å†³æ—©æœŸ Python å†…å­˜ç®¡ç†çš„çº¿ç¨‹å®‰å…¨é—®é¢˜ã€‚åœ¨é‚£ä¸ªå¹´ä»£ï¼Œå¤šæ ¸å¤„ç†å™¨è¿˜ä¸æ™®åŠï¼Œå•çº¿ç¨‹æ‰§è¡Œæ˜¯ä¸»æµã€‚GIL å¤§å¤§ç®€åŒ–äº† Python çš„å†…å­˜ç®¡ç†ï¼Œç‰¹åˆ«æ˜¯å¼•ç”¨è®¡æ•°æœºåˆ¶çš„å®ç°ã€‚ä¸éœ€è¦å¤æ‚çš„é”æœºåˆ¶æ¥ä¿æŠ¤æ¯ä¸ªå¯¹è±¡ï¼Œä¸€ä¸ªå…¨å±€é”å°±è§£å†³äº†çº¿ç¨‹å®‰å…¨é—®é¢˜ã€‚ä½¿å¾— C æ‰©å±•çš„ç¼–å†™æ›´å®¹æ˜“ï¼Œä¸éœ€è¦è€ƒè™‘å¤æ‚çš„çº¿ç¨‹åŒæ­¥é—®é¢˜ã€‚

è¿™ç§è®¾è®¡çš„ä¼˜ç‚¹:

1. å®ç°ç®€å•ä¸”å¯é ï¼šå•çº¿ç¨‹æ‰§è¡Œä¿è¯äº†å†…å­˜ç®¡ç†çš„å®‰å…¨æ€§ï¼Œå‡å°‘äº†æ­»é”ç­‰å¹¶å‘ bug çš„å¯èƒ½æ€§ï¼Œç®€åŒ–äº† C æ‰©å±•çš„å¼€å‘ã€‚

2. å¯¹äº I/O å¯†é›†å‹åº”ç”¨å½±å“è¾ƒå°ï¼š**Python åœ¨è¿›è¡Œ I/O æ“ä½œæ—¶ä¼šé‡Šæ”¾ GILï¼Œæ‰€ä»¥å¯¹äºç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶è¯»å†™ç­‰åœºæ™¯ï¼Œå¤šçº¿ç¨‹ä»ç„¶å¯ä»¥æä¾›æ€§èƒ½æå‡**ï¼Œå¤šçº¿ç¨‹ I/O æ˜¯ä¸ªå¾ˆå®åœ¨çš„éœ€æ±‚ã€‚

3. å•çº¿ç¨‹æ€§èƒ½æ›´å¥½ï¼šæ²¡æœ‰çº¿ç¨‹åˆ‡æ¢å¼€é”€ï¼Œä¸éœ€è¦å¤æ‚çš„é”æœºåˆ¶ï¼Œå†…å­˜ç®¡ç†æ•ˆç‡æ›´é«˜ã€‚


ç¼ºç‚¹:

1. æ— æ³•å……åˆ†åˆ©ç”¨å¤šæ ¸ CPUï¼šåŒä¸€æ—¶åˆ»åªèƒ½æ‰§è¡Œä¸€ä¸ªçº¿ç¨‹ã€‚

2. åœ¨è®¡ç®—å¯†é›†å‹ä»»åŠ¡ä¸­æ€§èƒ½å—é™ï¼šå³ä½¿æœ‰å¤šä¸ª CPU æ ¸å¿ƒä¹Ÿæ— æ³•å®ç°çœŸæ­£çš„å¹¶è¡Œè®¡ç®—ï¼Œæ‰€ä»¥**Python å¤„ç†è®¡ç®—å¯†é›†å‹ä»»åŠ¡éœ€è¦ç”¨å¤šè¿›ç¨‹**ï¼Œæ¯”å¦‚ä¸‹ä¾‹ï¼š

```python
# è®¡ç®—å¯†é›†å‹ä»»åŠ¡åœ¨å¤šçº¿ç¨‹ä¸‹å¯èƒ½æ¯”å•çº¿ç¨‹æ›´æ…¢
def compute_intensive():
    for i in range(10000000):
        x = i * i
        
# å¤šçº¿ç¨‹ç‰ˆæœ¬å¯èƒ½æ¯”å•çº¿ç¨‹æ›´æ…¢
threads = [Thread(target=compute_intensive) for _ in range(4)]
```

è§£å†³æ–¹æ¡ˆï¼š

1. åœ¨è®¡ç®—å¯†é›†å‹ä»»åŠ¡ä¸­ä½¿ç”¨å¤šè¿›ç¨‹æ›¿ä»£å¤šçº¿ç¨‹ï¼š

```python
from multiprocessing import Process # or use mp.spawn

# ä½¿ç”¨å¤šè¿›ç¨‹å¯ä»¥ç»•è¿‡ GIL é™åˆ¶
processes = [Process(target=compute_intensive) for _ in range(4)]
```
2. ä½¿ç”¨å…¶ä»– Python å®ç°ï¼Œæˆ–è€…æ›´é«˜ç‰ˆæœ¬çš„ [Python 3.12](https://www.reddit.com/r/Python/comments/1bcggx9/disabling_the_gil_option_has_been_merged_into/)ã€‚
  
3. å°†è®¡ç®—å¯†é›†å‹ä»»åŠ¡ç”¨ C/C++ å®ç°ï¼šé€šè¿‡æ‰©å±•æ¨¡å—æ–¹å¼ä½¿ç”¨ï¼Œåœ¨ C ä»£ç ä¸­å¯ä»¥é‡Šæ”¾ GILã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶ GIL æœ‰è¿™äº›é™åˆ¶ï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€ Python ä¸é€‚åˆå¼€å‘å¤§å‹åº”ç”¨ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼š

1. å¤§å¤šæ•°åº”ç”¨æ˜¯ I/O å¯†é›†å‹è€Œä¸æ˜¯ CPU å¯†é›†å‹ï¼ŒGIL çš„å½±å“æœ‰é™ã€‚

2. å¯ä»¥é€šè¿‡åˆé€‚çš„æ¶æ„è®¾è®¡è§„é¿ GIL çš„é™åˆ¶ï¼šä½¿ç”¨å¤šè¿›ç¨‹æ¶æ„ï¼Œä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹ï¼Œå°†è®¡ç®—å¯†é›†å‹ä»»åŠ¡äº¤ç»™ä¸“é—¨çš„æœåŠ¡ï¼Œæˆ–è°ƒç”¨C/C++æ¥å£ã€‚


## Ring / Tree Algorithm
- NCCLåœ¨å¯åŠ¨Collective Communicationå‰ä¼šæ ¹æ®ç½‘ç»œé€šä¿¡æ‹“æ‰‘benchmarkä¸åŒç®—æ³•ï¼Œå¹¶é€‰æ‹©å»¶è¿Ÿæœ€ä½çš„. Ringå’ŒTreeæ˜¯NCCLä¸­æœ€å¸¸è§çš„ä¸¤ç§æ‹“æ‰‘ç®—æ³•ï¼Œå¸¸åº”ç”¨äº![All-Reduce](./complete-allreduce.svg)ï¼Œä½†ä¹Ÿä¼šè¢«å…¶ä»–ç®—å­(Ring All-Gather, All-to-All)ä½¿ç”¨ã€‚
- æ›´å¤æ‚çš„ç®—æ³•æœ‰[SHARP](https://network.nvidia.com/pdf/solutions/hpc/paperieee_copyright.pdf)ï¼Œä¸€ç§multicastçš„å»¶ä¼¸ç®—æ³•ï¼ˆin-network reductionï¼‰ï¼Œä½¿ç”¨ç½‘ç»œswtiches(e.g. [NVSwitch](https://github.com/NVIDIA/nccl/issues/807)) ä¸Šçš„å¤„ç†å™¨æ‰§è¡Œreductionæ¥é¿å…offloadæ•°æ®åˆ°CPU/GPUä¸Šï¼Œå‡å°‘å»¶è¿Ÿå’ŒSMå ç”¨ã€‚
- ä»¥ä¸‹åˆ†æä¸­, nä¸ºå‚ä¸è®¡ç®—çš„GPUæ•°é‡ã€‚NVIDIAé€šå¸¸ä¸€ä¸ªèŠ‚ç‚¹(node)æœ‰8ä¸ªGPUï¼Œç”±é«˜å¸¦å®½NVLINK + NVSwitch densely connectæˆä¸€ä¸ªcomplete graphç½‘ç»œæ‹“æ‰‘

Claude ç”»çš„çŸ¢é‡å›¾å±å®æ— æ•Œäº†...

è¿™å¼ å›¾ä¸€ç›®äº†ç„¶èƒ½å¤Ÿç†è§£ Ring / Tree ç®—æ³•ã€‚

### Ring Algorithm

**ä¼˜ç‚¹:**

- å¸¦å®½åˆ©ç”¨ç‡é«˜: æ¯ä¸ªèŠ‚ç‚¹åŒæ—¶æ¥æ”¶å’Œå‘é€æ•°æ®,èƒ½å……åˆ†åˆ©ç”¨ç¡¬ä»¶å¸¦å®½
- è´Ÿè½½å‡è¡¡: æ¯ä¸ªèŠ‚ç‚¹å¤„ç†ç›¸åŒæ•°é‡çš„æ•°æ®,ç½‘ç»œè´Ÿè½½åˆ†å¸ƒå‡åŒ€
- å®ç°ç®€å•: å®¹é”™å’ŒåŒæ­¥æœºåˆ¶ç›¸å¯¹ç›´è§‚

**ç¼ºç‚¹:**

- å»¶è¿Ÿä¸èŠ‚ç‚¹æ•°å‘ˆçº¿æ€§å…³ç³»: å®Œæˆä¸€æ¬¡ AllReduce éœ€è¦ 2(n-1) æ­¥
- ä¸é€‚åˆå¤§è§„æ¨¡é›†ç¾¤: åœ¨æ•°åƒèŠ‚ç‚¹è§„æ¨¡ä¸‹,çº¿æ€§å¢é•¿çš„å»¶è¿Ÿä¼šæ˜¾è‘—å½±å“æ€§èƒ½, ä¸”å°‘æ•°ç¼“æ…¢stragglerèŠ‚ç‚¹å®¹æ˜“æˆä¸ºé€šä¿¡ç“¶é¢ˆ.
- å¯¹å°æ•°æ®ä¼ è¾“æ•ˆç‡ä¸é«˜: å¯åŠ¨å¼€é”€ç›¸å¯¹è¾ƒå¤§

**æ‹“å±•/åº”ç”¨:**
- NCCLåœ¨å•èŠ‚ç‚¹/èŠ‚ç‚¹æ•°å°‘æ—¶æ›´å®¹æ˜“é€‰æ‹©Ringç®—æ³•ï¼Œ[å¹¶ä¸ä¼šæ··ç”¨Ringå’ŒTree](https://github.com/NVIDIA/nccl/issues/471)ï¼ˆä¹Ÿè®¸æ˜¯æ‡’/ä¸ºäº†å®ç°å’Œbenchmarkç®€å•:( .
- å¯ç”¨Double/2D Ring Topologyé«˜æ•ˆåˆ©ç”¨èŠ‚ç‚¹å†…ï¼ˆintra-nodeï¼‰å¸¦å®½ï¼Œæ©ç›–/ç¼“è§£èŠ‚ç‚¹é—´ï¼ˆinter-nodeï¼‰é€šä¿¡å»¶è¿Ÿã€‚NCCLå¹¶æœªé‡‡ç”¨2D ring, ä½†è®ºæ–‡ [LoongTrain](https://arxiv.org/abs/2406.18485)ç”¨äº†2D ringæ¥åŠ é€ŸRing Attentionã€‚

### Tree Algorithm

ä¼ ç»Ÿçš„ Tree Algorithm çš„å»¶è¿Ÿä¸èŠ‚ç‚¹æ•°å‘ˆå¯¹æ•°å…³ç³»ã€‚å‚è€ƒä¸Šå›¾ï¼Œä¸€ç›®äº†ç„¶ã€‚

**ä¼˜ç‚¹**

- å»¶è¿Ÿä½ï¼šä¸èŠ‚ç‚¹æ•°å‘ˆå¯¹æ•°å…³ç³»: å®Œæˆé€šä¿¡åªéœ€ O(log n) æ­¥
- é€‚åˆå¤§è§„æ¨¡é›†ç¾¤/èŠ‚ç‚¹é—´é€šä¿¡: åœ¨å¤§è§„æ¨¡åœºæ™¯(å¦‚ 24000+ GPU)ä¸‹è¡¨ç°ä¼˜å¼‚

**ç¼ºç‚¹**

- å®ç°å¤æ‚: éœ€è¦ç»´æŠ¤ä¸¤æ£µäº’è¡¥çš„äºŒå‰æ ‘ç»“æ„
- å°è§„æ¨¡åœºæ™¯ä¼˜åŠ¿ä¸æ˜æ˜¾: åœ¨èŠ‚ç‚¹æ•°è¾ƒå°‘æ—¶,é¢å¤–çš„æ ‘ç»“æ„ç»´æŠ¤å¼€é”€å¯èƒ½å¾—ä¸å¿å¤±
- å¯¹ç½‘ç»œæ‹“æ‰‘ç»“æ„è¦æ±‚è¾ƒé«˜: éœ€è¦è‰¯å¥½çš„ç½‘ç»œäº’è”ä»¥æ”¯æŒæ ‘å½¢é€šä¿¡
- å¸¦å®½åˆ©ç”¨ç‡ä¸å¦‚Ring

**æ‹“å±•/åº”ç”¨**
- åºåˆ—å¹¶è¡Œç®—æ³•[Tree Attention](https://arxiv.org/abs/2408.04093) ä½¿ç”¨Tree All-Reduceæ¥åŠ é€Ÿæ¨ç†æ—¶çš„long-context attentionè®¡ç®—ï¼Œæ¯”Ring Attentionæ›´scalable, ä½†ç”±äºéš¾ä»¥overlapè®¡ç®—å’Œé€šè®¯ä¸é€‚ç”¨äºè®­ç»ƒã€‚

### Double Binary Tree Algorithm

ä» NCCL 2.4 ç‰ˆæœ¬å¼€å§‹ï¼Œå¯¹äºnodeæ•°è¾ƒå¤šçš„è·¨èŠ‚ç‚¹é€šä¿¡ä½¿ç”¨ [Double Binary Tree](https://developer.nvidia.com/blog/massively-scale-deep-learning-training-nccl-2-4/) ç®—æ³•ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„ Tree Algorithmï¼Œæ„é€ äº†ä¸¤æ£µäº’è¡¥çš„äºŒå‰æ ‘ç”¨äºå¹³è¡¡é€šä¿¡å¼€é”€ã€‚
![Double Binary Tree](./DBTree.jpg)
1. äº’è¡¥ç»“æ„ï¼šæ¯ä¸ªèŠ‚ç‚¹åœ¨ä¸€æ£µæ ‘ä¸­æ˜¯å†…éƒ¨èŠ‚ç‚¹ï¼ˆå‚ä¸æ•°æ®å‘é€å’Œè®¡ç®—ï¼‰ï¼Œåœ¨å¦ä¸€æ£µæ ‘ä¸­æ˜¯å¶å­èŠ‚ç‚¹ï¼ˆåªå‚ä¸æ•°æ®æ¥æ”¶ï¼‰ï¼Œç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹çš„å·¥ä½œè´Ÿè½½å¤§è‡´ç›¸åŒã€‚


2. æ•°æ®åˆ†å‰²ï¼šå°†éœ€è¦ä¼ è¾“çš„æ•°æ®åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œæ¯æ£µæ ‘è´Ÿè´£å¤„ç†ä¸€åŠçš„æ•°æ®ï¼Œä¸¤æ£µæ ‘å¹¶è¡Œå·¥ä½œã€‚

**ä¼˜ç‚¹**

- è§£å†³å¸¦å®½ç“¶é¢ˆï¼šé€šè¿‡åŒæ ‘ç»“æ„é¿å…äº†å•ç‚¹ç“¶é¢ˆ
- è´Ÿè½½å‡è¡¡ï¼šæ¯ä¸ªèŠ‚ç‚¹åœ¨ä¸¤æ£µæ ‘ä¸­äº¤æ›¿è§’è‰²ï¼Œä¿è¯è´Ÿè½½å‡è¡¡
- å»¶è¿Ÿä¼˜åŠ¿ï¼šä¿æŒäº† O(log n) çš„é€šä¿¡æ­¥æ•°
- é«˜å¯æ‰©å±•æ€§ï¼šé€‚åˆå¤§è§„æ¨¡é›†ç¾¤ï¼ˆ24000+ GPUï¼‰
- å®¹é”™æ€§å¥½ï¼šå•ä¸ªèŠ‚ç‚¹æ•…éšœå½±å“èŒƒå›´å°
- å¸¦å®½åˆ©ç”¨ç‡é«˜ï¼šé€šè¿‡æ•°æ®åˆ†æµå……åˆ†åˆ©ç”¨ç½‘ç»œå¸¦å®½

**ç¼ºç‚¹**

- å®ç°å¤æ‚ï¼šéœ€è¦ç»´æŠ¤ä¸¤æ£µäº’è¡¥äºŒå‰æ ‘
- é¢å¤–å¼€é”€ï¼šç»“æ„ç»´æŠ¤å’ŒåŒæ­¥å¼€é”€è¾ƒå¤§
- å°è§„æ¨¡åŠ£åŠ¿ï¼šèŠ‚ç‚¹æ•°å°‘æ—¶å¼€é”€å¯èƒ½å¾—ä¸å¿å¤±
- ç½‘ç»œæ•æ„Ÿï¼šå¯¹ç½‘ç»œè´¨é‡å’Œæ‹“æ‰‘ç»“æ„è¦æ±‚é«˜
- è°ƒè¯•å›°éš¾ï¼šåŒæ ‘ç»“æ„å¢åŠ äº†è°ƒè¯•å¤æ‚åº¦

### ä½¿ç”¨å»ºè®®

1. **å°è§„æ¨¡é›†ç¾¤ (< 32 GPU)**
   - æ¨èï¼šä¼ ç»Ÿ Tree Algorithm
   - åŸå› ï¼šå®ç°ç®€å•ï¼Œå¼€é”€å°ï¼Œæ€§èƒ½è¶³å¤Ÿ

2. **ä¸­ç­‰è§„æ¨¡ (32-512 GPU)**
   - éœ€è¦æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©ï¼š
   - æ³¨é‡ç®€å•ç¨³å®šï¼šä¼ ç»Ÿ Tree
   - æ³¨é‡æ€§èƒ½æ‰©å±•ï¼šDouble Binary Tree


3. **å¤§è§„æ¨¡é›†ç¾¤ (> 512 GPU)**
   - æ¨èï¼šDouble Binary Tree
   - åŸå› ï¼šæ›´å¥½çš„å¯æ‰©å±•æ€§å’Œè´Ÿè½½å‡è¡¡


### æ€§èƒ½å¯¹æ¯”

ä»¥ NVIDIA çš„æµ‹è¯•æ•°æ®ä¸ºä¾‹ï¼Œåœ¨ 24756 ä¸ª GPU çš„é›†ç¾¤ä¸­ï¼š

- Ring Algorithm: å»¶è¿Ÿçº¦ 180ms
- Tree Algorithm: å»¶è¿Ÿçº¦ 1ms
- æ€§èƒ½å·®è·æ¥è¿‘ 180 å€

## æ‹“å±•èµ„æ–™
- NCCLæ‹“æ‰‘benchmarkå’Œé€‰æ‹©: 
    - https://zhuanlan.zhihu.com/p/718639633 
    - è®¾ç½®ç¯å¢ƒå˜é‡æŸ¥çœ‹NCCL initæ—¶æ‹“æ‰‘benchmarkç»“æœï¼ˆè¾“å‡ºä¸€ä¸ªè¡¨æ ¼: latency/bandwidth): `NCCL_DEBUG=INFO NCCL_DEBUG_SUBSYS=INIT,ENV,TUNING`
    - è®¾ç½®ç¯å¢ƒå˜é‡æ§åˆ¶NCCLæ‹“æ‰‘ç®—æ³•: `NCCL_ALGO=TREE` æˆ– `NCCL_ALGO=RING`
- All-gather ç®—æ³•æ‹“æ‰‘ï¼š https://github.com/NVIDIA/nccl/issues/1123
- æ‰‹åŠ¨è·‘NCCLæ€§èƒ½benchmark: https://github.com/NVIDIA/nccl/issues/569
- SHARPç®—æ³•çš„æ€§èƒ½ä¼˜åŠ¿:
    - https://www.hpcuserforum.com/presentations/swiss/MellanoxHPCTechnology.pdf
    - https://www.youtube.com/watch?v=is7aBZ1_Op0
- å¤šæœºç¯å¢ƒä¸‹ï¼Œå¯ç”¨ `ibstatus` æŸ¥çœ‹Infinibandç½‘å¡çŠ¶æ€
- è®¾ç½® `NCCL_MAX_NCHANNELS=1`å¯é™åˆ¶cpu issue kernelåˆ°gpuçš„é€šé“æ•°ä¸º1ï¼ˆGPUç«¯scheduler launch kernelåç…§ä¸åŒCUDA streamå¹¶è¡Œæ‰§è¡Œï¼‰, ä»¥ä¿è¯kernel launchçš„é¡ºåºå’Œcpuç«¯è°ƒåº¦ä¸€è‡´ï¼Œé¿å…é€šè®¯kernelå…ˆå¯åŠ¨ï¼ŒæŠ¢å è®¡ç®—kernel SMådelayå…¶è¿è¡Œï¼Œæ— æ³•overlapã€‚
    - https://forums.developer.nvidia.com/t/how-many-streams-maximum-number-of-streams/6571/6
    - https://zhuanlan.zhihu.com/p/706805407
